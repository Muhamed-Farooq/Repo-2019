{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe856fddb38>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_datareader import data\n",
    "#pip install pandas-datareader\n",
    "\n",
    "stock = 'MGLU3.SA'\n",
    "source = 'yahoo'\n",
    "\n",
    "# Set date range (Google went public August 19, 2004)\n",
    "start = datetime.datetime(2004, 8, 19)\n",
    "end = datetime.datetime(2019, 7, 19)\n",
    "\n",
    "# Collect Google stock data\n",
    "goog_df = data.DataReader(stock, source, start, end)\n",
    "\n",
    "dataset = goog_df['Adj Close']\n",
    "print(len(dataset))\n",
    "goog_df['Adj Close'].plot(kind='line', grid=True, title='GOOG Adjusted Closes, IPO through 2016')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1629 408\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "dataset = np.array(dataset.astype('float32')).reshape(-1,1)\n",
    "\n",
    "def norm(x):\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "#dataset=norm(dataset)\n",
    "\n",
    "look_back=2\n",
    "np.random.seed(7)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))\n",
    "\n",
    "def create_dataset(dataset, look_back=look_back):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "trainX\n",
    "\n",
    "trainY = trainY.reshape(len(trainY), 1)\n",
    "testY = testY.reshape(len(testY), 1)\n",
    "trainY\n",
    "\n",
    "X0=trainX\n",
    "Y0=trainY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 2, 1), dtype=float32)\n",
      "All parameters: 328200.0\n",
      "Trainable parameters: 109446\n",
      "Step 1, Minibatch Loss= 0.0040, Training Accuracy= 0.9367, Test Accuracy= 0.4366\n",
      "Step 10, Minibatch Loss= 0.0011, Training Accuracy= 0.9662, Test Accuracy= 0.5428\n",
      "Step 20, Minibatch Loss= 0.0001, Training Accuracy= 0.9895, Test Accuracy= 0.6565\n",
      "Step 30, Minibatch Loss= 0.0001, Training Accuracy= 0.9910, Test Accuracy= 0.6845\n",
      "Step 40, Minibatch Loss= 0.0000, Training Accuracy= 0.9934, Test Accuracy= 0.7259\n",
      "Step 50, Minibatch Loss= 0.0000, Training Accuracy= 0.9946, Test Accuracy= 0.7153\n",
      "Step 60, Minibatch Loss= 0.0000, Training Accuracy= 0.9951, Test Accuracy= 0.7288\n",
      "Step 70, Minibatch Loss= 0.0000, Training Accuracy= 0.9955, Test Accuracy= 0.7264\n",
      "Step 80, Minibatch Loss= 0.0000, Training Accuracy= 0.9957, Test Accuracy= 0.7320\n",
      "Step 90, Minibatch Loss= 0.0000, Training Accuracy= 0.9958, Test Accuracy= 0.7359\n",
      "Step 100, Minibatch Loss= 0.0000, Training Accuracy= 0.9959, Test Accuracy= 0.7383\n",
      "Step 110, Minibatch Loss= 0.0000, Training Accuracy= 0.9962, Test Accuracy= 0.7418\n",
      "Step 120, Minibatch Loss= 0.0000, Training Accuracy= 0.9963, Test Accuracy= 0.7454\n",
      "Step 130, Minibatch Loss= 0.0000, Training Accuracy= 0.9964, Test Accuracy= 0.7476\n",
      "Step 140, Minibatch Loss= 0.0000, Training Accuracy= 0.9966, Test Accuracy= 0.7464\n",
      "Step 150, Minibatch Loss= 0.0000, Training Accuracy= 0.9966, Test Accuracy= 0.7486\n",
      "Step 160, Minibatch Loss= 0.0000, Training Accuracy= 0.9966, Test Accuracy= 0.7488\n",
      "Step 170, Minibatch Loss= 0.0000, Training Accuracy= 0.9964, Test Accuracy= 0.7527\n",
      "Step 180, Minibatch Loss= 0.0000, Training Accuracy= 0.9966, Test Accuracy= 0.7518\n",
      "Step 190, Minibatch Loss= 0.0000, Training Accuracy= 0.9967, Test Accuracy= 0.7496\n",
      "Step 200, Minibatch Loss= 0.0000, Training Accuracy= 0.9967, Test Accuracy= 0.7504\n",
      "Step 210, Minibatch Loss= 0.0000, Training Accuracy= 0.9966, Test Accuracy= 0.7490\n",
      "Step 220, Minibatch Loss= 0.0000, Training Accuracy= 0.9967, Test Accuracy= 0.7522\n",
      "Step 230, Minibatch Loss= 0.0000, Training Accuracy= 0.9967, Test Accuracy= 0.7516\n",
      "Step 240, Minibatch Loss= 0.0000, Training Accuracy= 0.9965, Test Accuracy= 0.7484\n",
      "Step 250, Minibatch Loss= 0.0000, Training Accuracy= 0.9966, Test Accuracy= 0.7550\n",
      "Step 260, Minibatch Loss= 0.0000, Training Accuracy= 0.9966, Test Accuracy= 0.7509\n",
      "Step 270, Minibatch Loss= 0.0000, Training Accuracy= 0.9967, Test Accuracy= 0.7529\n",
      "Step 280, Minibatch Loss= 0.0000, Training Accuracy= 0.9967, Test Accuracy= 0.7515\n",
      "Step 290, Minibatch Loss= 0.0000, Training Accuracy= 0.9967, Test Accuracy= 0.7516\n",
      "Step 300, Minibatch Loss= 0.0000, Training Accuracy= 0.9965, Test Accuracy= 0.7570\n",
      "Step 310, Minibatch Loss= 0.0000, Training Accuracy= 0.9967, Test Accuracy= 0.7531\n",
      "Step 320, Minibatch Loss= 0.0000, Training Accuracy= 0.9967, Test Accuracy= 0.7530\n",
      "Step 330, Minibatch Loss= 0.0000, Training Accuracy= 0.9964, Test Accuracy= 0.7577\n",
      "Step 340, Minibatch Loss= 0.0000, Training Accuracy= 0.9967, Test Accuracy= 0.7555\n",
      "Step 350, Minibatch Loss= 0.0000, Training Accuracy= 0.9965, Test Accuracy= 0.7496\n",
      "Step 360, Minibatch Loss= 0.0000, Training Accuracy= 0.9968, Test Accuracy= 0.7521\n",
      "Step 370, Minibatch Loss= 0.0000, Training Accuracy= 0.9964, Test Accuracy= 0.7578\n",
      "Step 380, Minibatch Loss= 0.0000, Training Accuracy= 0.9966, Test Accuracy= 0.7501\n",
      "Step 390, Minibatch Loss= 0.0000, Training Accuracy= 0.9968, Test Accuracy= 0.7518\n",
      "Step 400, Minibatch Loss= 0.0000, Training Accuracy= 0.9963, Test Accuracy= 0.7573\n",
      "Step 410, Minibatch Loss= 0.0000, Training Accuracy= 0.9968, Test Accuracy= 0.7530\n",
      "Step 420, Minibatch Loss= 0.0000, Training Accuracy= 0.9968, Test Accuracy= 0.7520\n",
      "Step 430, Minibatch Loss= 0.0000, Training Accuracy= 0.9967, Test Accuracy= 0.7529\n",
      "Step 440, Minibatch Loss= 0.0000, Training Accuracy= 0.9965, Test Accuracy= 0.7483\n",
      "Step 450, Minibatch Loss= 0.0000, Training Accuracy= 0.9963, Test Accuracy= 0.7472\n",
      "Step 460, Minibatch Loss= 0.0000, Training Accuracy= 0.9966, Test Accuracy= 0.7540\n",
      "Step 470, Minibatch Loss= 0.0000, Training Accuracy= 0.9962, Test Accuracy= 0.7557\n",
      "Step 480, Minibatch Loss= 0.0000, Training Accuracy= 0.9968, Test Accuracy= 0.7510\n",
      "Step 490, Minibatch Loss= 0.0000, Training Accuracy= 0.9961, Test Accuracy= 0.7452\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ae31c74dcd98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-ae31c74dcd98>\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(num, data, labels)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mdata_shuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mlabels_shuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_shuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_shuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/rubensvectomobile/BOVESPA/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tfd = tfp.distributions\n",
    "\n",
    "class TemporalConvNet(tf.layers.Layer):\n",
    "    def __init__(self, num_channels, kernel_size=2, dropout=0.2,\n",
    "                 trainable=True, name=None, dtype=None, \n",
    "                 activity_regularizer=None, **kwargs):\n",
    "        super(TemporalConvNet, self).__init__(\n",
    "            trainable=trainable, dtype=dtype,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            name=name, **kwargs\n",
    "        )\n",
    "        self.layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            out_channels = num_channels[i]\n",
    "            self.layers.append(\n",
    "                TemporalBlock(out_channels, kernel_size, strides=1, dilation_rate=dilation_size,\n",
    "                              dropout=dropout, name=\"tblock_{}\".format(i))\n",
    "            )\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        outputs = inputs\n",
    "        for layer in self.layers:\n",
    "            outputs = layer(outputs, training=training)\n",
    "        return outputs\n",
    "\n",
    "learning_rate = 0.001\n",
    "display_step = 10\n",
    "num_input = 1\n",
    "num_hidden = 35\n",
    "num_classes = 1\n",
    "\n",
    "dropout = 0\n",
    "kernel_size = 8\n",
    "levels = 6\n",
    "\n",
    "class CausalConv1D(tf.layers.Conv1D):\n",
    "    def __init__(self, filters,\n",
    "               kernel_size,\n",
    "               strides=1,\n",
    "               dilation_rate=1,\n",
    "               activation=None,\n",
    "               use_bias=True,\n",
    "               kernel_initializer=None,\n",
    "               bias_initializer=tf.zeros_initializer(),\n",
    "               kernel_regularizer=None,\n",
    "               bias_regularizer=None,\n",
    "               activity_regularizer=None,\n",
    "               kernel_constraint=None,\n",
    "               bias_constraint=None,\n",
    "               trainable=True,\n",
    "               name=None,\n",
    "               **kwargs):\n",
    "        super(CausalConv1D, self).__init__(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding='valid',\n",
    "            data_format='channels_last',\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            trainable=trainable,\n",
    "            name=name, **kwargs\n",
    "        )\n",
    "       \n",
    "    def call(self, inputs):\n",
    "        padding = (self.kernel_size[0] - 1) * self.dilation_rate[0]\n",
    "        inputs = tf.pad(inputs, tf.constant([(0, 0,), (1, 0), (0, 0)]) * padding)\n",
    "        return super(CausalConv1D, self).call(inputs)\n",
    "\n",
    "\n",
    "\n",
    "class TemporalBlock(tf.layers.Layer):\n",
    "    def __init__(self, n_outputs, kernel_size, strides, dilation_rate, dropout=0.1, \n",
    "                 trainable=True, name=None, dtype=None, \n",
    "                 activity_regularizer=None, **kwargs):\n",
    "        super(TemporalBlock, self).__init__(\n",
    "            trainable=trainable, dtype=dtype,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            name=name, **kwargs\n",
    "        )        \n",
    "        self.dropout = dropout\n",
    "        self.n_outputs = n_outputs\n",
    "        self.conv1 = CausalConv1D(\n",
    "            n_outputs, kernel_size, strides=strides, \n",
    "            dilation_rate=dilation_rate, activation=tf.nn.relu, \n",
    "            name=\"conv1\")\n",
    "        self.conv2 = CausalConv1D(\n",
    "            n_outputs, kernel_size, strides=strides, \n",
    "            dilation_rate=dilation_rate, activation=tf.nn.relu, \n",
    "            name=\"conv2\")\n",
    "        self.down_sample = None\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        channel_dim = 2\n",
    "        self.dropout1 = tf.layers.Dropout(self.dropout, [tf.constant(1), tf.constant(1), tf.constant(self.n_outputs)])\n",
    "        self.dropout2 = tf.layers.Dropout(self.dropout, [tf.constant(1), tf.constant(1), tf.constant(self.n_outputs)])\n",
    "        if input_shape[channel_dim] != self.n_outputs:\n",
    "            self.down_sample = tf.layers.Dense(self.n_outputs, activation=None)\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        x = self.conv1(inputs)\n",
    "        x = tf.contrib.layers.layer_norm(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.conv2(x)\n",
    "        x = tf.contrib.layers.layer_norm(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        #x = tfp.layers.DistributionLambda(make_distribution_fn=lambda t: tfd.Normal(loc=t, scale=1))(x)\n",
    "        #x = tf.contrib.layers.layer_norm(x)\n",
    "        #x = tf.layers.dense(inputs=x,units=3)\n",
    "        if self.down_sample is not None:\n",
    "          inputs = self.down_sample(inputs)\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.set_random_seed(2)\n",
    "    \n",
    "    X = tf.placeholder(\"float\", [None, look_back,1])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    is_training = tf.placeholder(\"bool\")\n",
    "    \n",
    "    logits = tf.layers.dense(\n",
    "        TemporalConvNet([num_hidden] * levels, kernel_size, dropout)(\n",
    "            X, training=is_training),\n",
    "        num_classes, activation=None, \n",
    "        kernel_initializer=tf.glorot_uniform_initializer()\n",
    "    )\n",
    "    print(logits)\n",
    "\n",
    "    mm,_=tf.nn.moments(tf.reshape(tf.nn.relu(logits),[-1,look_back]),axes=[1])\n",
    "    prediction=tf.nn.relu(logits)\n",
    "    \n",
    "    prediction2 = tf.reshape(tf.cast(mm,tf.float32),[-1,1])\n",
    "    \n",
    "    loss_op = tf.reduce_mean(tf.losses.mean_squared_error(\n",
    "        labels=Y,predictions=prediction2))\n",
    "    \n",
    "    accuracy=1-tf.sqrt(loss_op)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    print(\"All parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.global_variables()]))\n",
    "    print(\"Trainable parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()]))\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "    return np.asarray(data_shuffle).astype(np.float32), np.asarray(labels_shuffle).astype(np.float32)\n",
    "\n",
    "log_dir = \"/home/rubensvectomobile/BOVESPA/\"\n",
    "tb_writer = tf.summary.FileWriter(log_dir, graph)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = False\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "best_val_acc = 0.75\n",
    "\n",
    "training_epochs = 2000\n",
    "batch_size = X0.shape[0]\n",
    "\n",
    "\n",
    "X0=X0.reshape(-1,look_back,1)\n",
    "testX=testX.reshape(-1,look_back,1)\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init)\n",
    "    for step in range(1, training_epochs+1):\n",
    "        Xt, Yt = next_batch(batch_size, X0, Y0)\n",
    "        batch_x, batch_y = Xt,Yt\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, is_training: True})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={\n",
    "                X: batch_x, Y: batch_y, is_training: False})\n",
    "            test_data = testX\n",
    "            test_label = testY\n",
    "            val_acc = sess.run(accuracy, feed_dict={X: test_data, Y: test_label, is_training: False})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.4f}\".format(acc) + \", Test Accuracy= \" + \\\n",
    "                  \"{:.4f}\".format(val_acc))\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                save_path = saver.save(sess, \"/home/rubensvectomobile/BOVESPA/model.ckpt\")\n",
    "                print(\"Model saved in path: %s\" % save_path)\n",
    "    pred00 = sess.run([prediction], feed_dict={X: test_data, is_training: False})\n",
    "    pred01 = sess.run([prediction2], feed_dict={X: test_data, is_training: False})\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    ckpt = \"/home/rubensvectomobile/BOVESPA/model.ckpt\"\n",
    "    saver.restore(session, ckpt)\n",
    "    pred00 = session.run([prediction], feed_dict={X: test_data, is_training: False})\n",
    "    pred01 = session.run([prediction2], feed_dict={X: test_data, is_training: False})\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array(pred01).reshape(-1,1))\n",
    "plt.plot(np.array(testY).reshape(-1,),c='black')\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()\n",
    "\n",
    "x=list(range(0,len(pred00[0])))\n",
    "y1=np.array(pred01).reshape(1,-1)[0]+2*np.std(np.array(pred01).reshape(1,-1))\n",
    "y2=np.array(pred01).reshape(1,-1)[0]-2*np.std(np.array(pred01).reshape(1,-1))\n",
    "fig, ax1 = plt.subplots(1, 1, sharex=True)\n",
    "ax1.plot(np.array(pred01).reshape(1,-1)[0],'--',color='blue',alpha=0.5)\n",
    "ax1.fill_between(x, y1, y2,color='blue',alpha=0.3)\n",
    "ax1.plot(x,np.array(testY).reshape(-1,),c='red')\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
