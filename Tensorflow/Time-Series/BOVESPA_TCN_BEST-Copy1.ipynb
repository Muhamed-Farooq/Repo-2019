{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fad0e289fd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_datareader import data\n",
    "#pip install pandas-datareader\n",
    "\n",
    "stock = 'MGLU3.SA'\n",
    "source = 'yahoo'\n",
    "\n",
    "# Set date range (Google went public August 19, 2004)\n",
    "start = datetime.datetime(2004, 8, 19)\n",
    "end = datetime.datetime(2019, 7, 19)\n",
    "\n",
    "# Collect Google stock data\n",
    "goog_df = data.DataReader(stock, source, start, end)\n",
    "\n",
    "dataset = goog_df['Adj Close']\n",
    "print(len(dataset))\n",
    "goog_df['Adj Close'].plot(kind='line', grid=True, title='GOOG Adjusted Closes, IPO through 2016')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 21\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "dataset = np.array(dataset.astype('float32')).reshape(-1,1)\n",
    "\n",
    "def norm(x):\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "#dataset=norm(dataset)\n",
    "\n",
    "look_back=2\n",
    "np.random.seed(7)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "train_size = int(len(dataset) * 0.99)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))\n",
    "\n",
    "def create_dataset(dataset, look_back=look_back):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "trainX\n",
    "\n",
    "trainY = trainY.reshape(len(trainY), 1)\n",
    "testY = testY.reshape(len(testY), 1)\n",
    "trainY\n",
    "\n",
    "X0=trainX\n",
    "Y0=trainY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rubensvectomobile_gmail_com/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 2, 1), dtype=float32)\n",
      "All parameters: 328340.0\n",
      "Trainable parameters: 109446\n",
      "Step 1, Minibatch Loss= 0.0633, Training Accuracy= 0.7485, Test Accuracy= 0.4845\n",
      "Step 10, Minibatch Loss= 0.0144, Training Accuracy= 0.8801, Test Accuracy= 0.4356\n",
      "Step 20, Minibatch Loss= 0.0048, Training Accuracy= 0.9307, Test Accuracy= 0.7105\n",
      "Step 30, Minibatch Loss= 0.0021, Training Accuracy= 0.9543, Test Accuracy= 0.7208\n",
      "Step 40, Minibatch Loss= 0.0012, Training Accuracy= 0.9650, Test Accuracy= 0.7801\n",
      "Model saved in path: /home/rubensvectomobile/BOVESPA/model.ckpt\n",
      "Step 50, Minibatch Loss= 0.0009, Training Accuracy= 0.9701, Test Accuracy= 0.8332\n",
      "Model saved in path: /home/rubensvectomobile/BOVESPA/model.ckpt\n",
      "Step 60, Minibatch Loss= 0.0009, Training Accuracy= 0.9707, Test Accuracy= 0.8787\n",
      "Model saved in path: /home/rubensvectomobile/BOVESPA/model.ckpt\n",
      "Step 70, Minibatch Loss= 0.0008, Training Accuracy= 0.9712, Test Accuracy= 0.8983\n",
      "Model saved in path: /home/rubensvectomobile/BOVESPA/model.ckpt\n",
      "Step 80, Minibatch Loss= 0.0008, Training Accuracy= 0.9714, Test Accuracy= 0.8993\n",
      "Model saved in path: /home/rubensvectomobile/BOVESPA/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "tfd = tfp.distributions\n",
    "\n",
    "class TemporalConvNet(tf.layers.Layer):\n",
    "    def __init__(self, num_channels, kernel_size=2, dropout=0.2,\n",
    "                 trainable=True, name=None, dtype=None, \n",
    "                 activity_regularizer=None, **kwargs):\n",
    "        super(TemporalConvNet, self).__init__(\n",
    "            trainable=trainable, dtype=dtype,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            name=name, **kwargs\n",
    "        )\n",
    "        self.layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            out_channels = num_channels[i]\n",
    "            self.layers.append(\n",
    "                TemporalBlock(out_channels, kernel_size, strides=1, dilation_rate=dilation_size,\n",
    "                              dropout=dropout, name=\"tblock_{}\".format(i))\n",
    "            )\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        outputs = inputs\n",
    "        for layer in self.layers:\n",
    "            outputs = layer(outputs, training=training)\n",
    "        return outputs\n",
    "\n",
    "learning_rate = 0.001\n",
    "display_step = 10\n",
    "num_input = 1\n",
    "num_hidden = 35\n",
    "num_classes = 1\n",
    "\n",
    "dropout = 0\n",
    "kernel_size = 8\n",
    "levels = 6\n",
    "\n",
    "class CausalConv1D(tf.layers.Conv1D):\n",
    "    def __init__(self, filters,\n",
    "               kernel_size,\n",
    "               strides=1,\n",
    "               dilation_rate=1,\n",
    "               activation=None,\n",
    "               use_bias=True,\n",
    "               kernel_initializer=None,\n",
    "               bias_initializer=tf.zeros_initializer(),\n",
    "               kernel_regularizer=None,\n",
    "               bias_regularizer=None,\n",
    "               activity_regularizer=None,\n",
    "               kernel_constraint=None,\n",
    "               bias_constraint=None,\n",
    "               trainable=True,\n",
    "               name=None,\n",
    "               **kwargs):\n",
    "        super(CausalConv1D, self).__init__(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding='valid',\n",
    "            data_format='channels_last',\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            trainable=trainable,\n",
    "            name=name, **kwargs\n",
    "        )\n",
    "       \n",
    "    def call(self, inputs):\n",
    "        padding = (self.kernel_size[0] - 1) * self.dilation_rate[0]\n",
    "        inputs = tf.pad(inputs, tf.constant([(0, 0,), (1, 0), (0, 0)]) * padding)\n",
    "        return super(CausalConv1D, self).call(inputs)\n",
    "\n",
    "\n",
    "\n",
    "class TemporalBlock(tf.layers.Layer):\n",
    "    def __init__(self, n_outputs, kernel_size, strides, dilation_rate, dropout=0.1, \n",
    "                 trainable=True, name=None, dtype=None, \n",
    "                 activity_regularizer=None, **kwargs):\n",
    "        super(TemporalBlock, self).__init__(\n",
    "            trainable=trainable, dtype=dtype,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            name=name, **kwargs\n",
    "        )        \n",
    "        self.dropout = dropout\n",
    "        self.n_outputs = n_outputs\n",
    "        self.conv1 = CausalConv1D(\n",
    "            n_outputs, kernel_size, strides=strides, \n",
    "            dilation_rate=dilation_rate, activation=tf.nn.relu, \n",
    "            name=\"conv1\")\n",
    "        self.conv2 = CausalConv1D(\n",
    "            n_outputs, kernel_size, strides=strides, \n",
    "            dilation_rate=dilation_rate, activation=tf.nn.relu, \n",
    "            name=\"conv2\")\n",
    "        self.down_sample = None\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        channel_dim = 2\n",
    "        self.dropout1 = tf.layers.Dropout(self.dropout, [tf.constant(1), tf.constant(1), tf.constant(self.n_outputs)])\n",
    "        self.dropout2 = tf.layers.Dropout(self.dropout, [tf.constant(1), tf.constant(1), tf.constant(self.n_outputs)])\n",
    "        if input_shape[channel_dim] != self.n_outputs:\n",
    "            self.down_sample = tf.layers.Dense(self.n_outputs, activation=None)\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        x = self.conv1(inputs)\n",
    "        x = tf.contrib.layers.layer_norm(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.conv2(x)\n",
    "        x = tf.contrib.layers.layer_norm(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        #x = tfp.layers.DistributionLambda(make_distribution_fn=lambda t: tfd.Normal(loc=t, scale=1))(x)\n",
    "        #x = tf.contrib.layers.layer_norm(x)\n",
    "        #x = tf.layers.dense(inputs=x,units=3)\n",
    "        if self.down_sample is not None:\n",
    "          inputs = self.down_sample(inputs)\n",
    "        return tf.nn.relu(x+inputs)\n",
    "\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.set_random_seed(2)\n",
    "    \n",
    "    X = tf.placeholder(\"float\", [None, look_back,1])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    is_training = tf.placeholder(\"bool\")\n",
    "    \n",
    "    logits = tf.layers.dense(\n",
    "        TemporalConvNet([num_hidden] * levels, kernel_size, dropout)(\n",
    "            X, training=is_training),\n",
    "        num_classes, activation=None, \n",
    "        kernel_initializer=tf.glorot_uniform_initializer()\n",
    "    )\n",
    "    print(logits)\n",
    "\n",
    "    mm,_=tf.nn.moments(tf.reshape(tf.nn.relu(logits),[-1,look_back]),axes=[1])\n",
    "    prediction=tf.nn.relu(logits)\n",
    "    \n",
    "    prediction2 = tf.reshape(tf.cast(mm,tf.float32),[-1,1])\n",
    "    \n",
    "    loss_op = tf.reduce_mean(tf.losses.mean_squared_error(\n",
    "        labels=Y,predictions=prediction2))\n",
    "    \n",
    "    accuracy=1-tf.sqrt(loss_op)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    print(\"All parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.global_variables()]))\n",
    "    print(\"Trainable parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()]))\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "    return np.asarray(data_shuffle).astype(np.float32), np.asarray(labels_shuffle).astype(np.float32)\n",
    "\n",
    "log_dir = \"/home/rubensvectomobile/BOVESPA/\"\n",
    "tb_writer = tf.summary.FileWriter(log_dir, graph)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = False\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "best_val_acc = 0.75\n",
    "\n",
    "training_epochs = 15000\n",
    "batch_size = X0.shape[0]\n",
    "\n",
    "\n",
    "X0=X0.reshape(-1,look_back,1)\n",
    "testX=testX.reshape(-1,look_back,1)\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init)\n",
    "    for step in range(1, training_epochs+1):\n",
    "        Xt, Yt = next_batch(batch_size, X0, Y0)\n",
    "        batch_x, batch_y = Xt,Yt\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, is_training: True})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={\n",
    "                X: batch_x, Y: batch_y, is_training: False})\n",
    "            test_data = testX\n",
    "            test_label = testY\n",
    "            val_acc = sess.run(accuracy, feed_dict={X: test_data, Y: test_label, is_training: False})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.4f}\".format(acc) + \", Test Accuracy= \" + \\\n",
    "                  \"{:.4f}\".format(val_acc))\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                save_path = saver.save(sess, \"/home/rubensvectomobile/BOVESPA/model.ckpt\")\n",
    "                print(\"Model saved in path: %s\" % save_path)\n",
    "    pred00 = sess.run([prediction], feed_dict={X: test_data, is_training: False})\n",
    "    pred01 = sess.run([prediction2], feed_dict={X: test_data, is_training: False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    ckpt = \"/home/rubensvectomobile/BOVESPA/model.ckpt\"\n",
    "    saver.restore(session, ckpt)\n",
    "    pred00 = session.run([prediction], feed_dict={X: test_data, is_training: False})\n",
    "    pred01 = session.run([prediction2], feed_dict={X: test_data, is_training: False})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array(pred01).reshape(-1,1))\n",
    "plt.plot(np.array(testY).reshape(-1,),c='black')\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()\n",
    "\n",
    "x=list(range(0,len(pred00[0])))\n",
    "y1=np.array(pred01).reshape(1,-1)[0]+2*np.std(np.array(pred01).reshape(1,-1))\n",
    "y2=np.array(pred01).reshape(1,-1)[0]-2*np.std(np.array(pred01).reshape(1,-1))\n",
    "fig, ax1 = plt.subplots(1, 1, sharex=True)\n",
    "ax1.plot(np.array(pred01).reshape(1,-1)[0],'--',color='blue',alpha=0.5)\n",
    "ax1.fill_between(x, y1, y2,color='blue',alpha=0.3)\n",
    "ax1.plot(x,np.array(testY).reshape(-1,),c='red')\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
