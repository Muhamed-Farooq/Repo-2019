{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/rubensvectomobile_gmail_com/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rubensvectomobile_gmail_com/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rubensvectomobile_gmail_com/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rubensvectomobile_gmail_com/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rubensvectomobile_gmail_com/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rubensvectomobile_gmail_com/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rubensvectomobile_gmail_com/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rubensvectomobile_gmail_com/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rubensvectomobile_gmail_com/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rubensvectomobile_gmail_com/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rubensvectomobile_gmail_com/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rubensvectomobile_gmail_com/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Average\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 2\n",
    "nb_epoch = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 20, 20\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 64\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "input_shape=(20, 20,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2771, 401)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vetor_atendente_final=pd.read_excel('Atendentes_Vetores.xlsx',header=0,index=False)\n",
    "vetor_atendente_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2771, 400)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor_cliente_final=pd.read_excel('Clientes_Vetores.xlsx',header=0,index=False)\n",
    "vetor_atendente_final.iloc[:,0:-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       ...,\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target=np.array(pd.get_dummies(vetor_atendente_final.iloc[:,-1]))\n",
    "target_=target\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm2(x):\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_dados=pd.read_excel('TabelaDados.xlsx',header=0,index=False)\n",
    "tabela_dados2=tabela_dados.iloc[np.concatenate([np.where(target.T[1]==1)[0][0:1680],np.where(target.T[1]==0)[0][-1091:]],axis=0),:].fillna(0.9)\n",
    "tabela_dados2['Satisfação cliente']=tabela_dados2['Satisfação cliente'].astype('category').cat.codes\n",
    "tabela_dados2['Satisfação atendente']=tabela_dados2['Satisfação atendente'].astype('category').cat.codes\n",
    "tabela_dados3=tabela_dados2[['Silencio','Tempo de Ligação','Atend_Lex','Cliente_Lex','Satisfação cliente',\n",
    "                                     'Satisfação atendente']]\n",
    "tabela_dados3=norm2(tabela_dados3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.700646\n",
       "1       0.701806\n",
       "2       0.062971\n",
       "3       0.533697\n",
       "4       0.068154\n",
       "          ...   \n",
       "2766    0.704984\n",
       "2767    0.067725\n",
       "2768    0.697560\n",
       "2769    0.678057\n",
       "2770    0.679891\n",
       "Name: Silencio, Length: 2771, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabela_dados3['Silencio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "2766    1\n",
       "2767    1\n",
       "2768    1\n",
       "2769    1\n",
       "2770    1\n",
       "Name: Silencio, Length: 2771, dtype: category\n",
       "Categories (7, int64): [1 < 2 < 3 < 4 < 5 < 6 < 7]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = [0, 15, 25, 35, 50, 75,85, 100]\n",
    "labels = [1,2,3,4,5,6,7]\n",
    "for i in tabela_dados3.columns[0:4]:\n",
    "    tabela_dados3[i] = pd.cut(tabela_dados3[i], bins=bins,labels=labels)\n",
    "tabela_dados3['Silencio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tabela_dados3=np.array(tabela_dados3.iloc[:,[2,3]])\n",
    "\n",
    "variaveis=tabela_dados3.shape[1]\n",
    "concat=np.array(tabela_dados3)\n",
    "concat=concat.reshape(-1,variaveis,1)\n",
    "concat;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2771, 20, 20, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def norm(x):\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "x_train0=np.array(norm(np.array(vetor_atendente_final.iloc[:,0:-1]))).reshape(-1,20,20,1)\n",
    "x_train1=np.array(norm(np.array(vetor_cliente_final.iloc[:,0:-1]))).reshape(-1,20,20,1)\n",
    "x_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = x_train0, target\n",
    "X1, y1 = x_train1, target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.1, random_state=42)\n",
    "concat_train,concat_test, y_train2, y_test2=train_test_split(concat, y1, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rubensvectomobile_gmail_com/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), input_shape=(20, 20, 1..., padding=\"same\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:46: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), kernel_initializer=\"glorot_uniform\", padding=\"same\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:49: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), kernel_initializer=\"glorot_uniform\", padding=\"same\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:53: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), kernel_initializer=\"glorot_uniform\", padding=\"same\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:57: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), kernel_initializer=\"glorot_uniform\", padding=\"same\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:60: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (3, 3), kernel_initializer=\"glorot_uniform\", padding=\"same\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), input_shape=(20, 20, 1..., padding=\"same\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:74: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), kernel_initializer=\"glorot_uniform\", padding=\"same\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), kernel_initializer=\"glorot_uniform\", padding=\"same\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:81: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), kernel_initializer=\"glorot_uniform\", padding=\"same\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:85: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), kernel_initializer=\"glorot_uniform\", padding=\"same\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:88: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (3, 3), kernel_initializer=\"glorot_uniform\", padding=\"same\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:119: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2243 samples, validate on 250 samples\n",
      "Epoch 1/600\n",
      "2243/2243 [==============================] - 2s 978us/step - loss: 0.7032 - acc: 0.3977 - val_loss: 0.6860 - val_acc: 0.5800\n",
      "Epoch 2/600\n",
      "2243/2243 [==============================] - 0s 82us/step - loss: 0.6826 - acc: 0.6068 - val_loss: 0.6801 - val_acc: 0.5800\n",
      "Epoch 3/600\n",
      "2243/2243 [==============================] - 0s 79us/step - loss: 0.6726 - acc: 0.6068 - val_loss: 0.6831 - val_acc: 0.5800\n",
      "Epoch 4/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6709 - acc: 0.6068 - val_loss: 0.6864 - val_acc: 0.5800\n",
      "Epoch 5/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6714 - acc: 0.6068 - val_loss: 0.6850 - val_acc: 0.5800\n",
      "Epoch 6/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6714 - acc: 0.6068 - val_loss: 0.6832 - val_acc: 0.5800\n",
      "Epoch 7/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6705 - acc: 0.6068 - val_loss: 0.6818 - val_acc: 0.5800\n",
      "Epoch 8/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6701 - acc: 0.6068 - val_loss: 0.6810 - val_acc: 0.5800\n",
      "Epoch 9/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6702 - acc: 0.6068 - val_loss: 0.6808 - val_acc: 0.5800\n",
      "Epoch 10/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6702 - acc: 0.6068 - val_loss: 0.6809 - val_acc: 0.5800\n",
      "Epoch 11/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6698 - acc: 0.6068 - val_loss: 0.6811 - val_acc: 0.5800\n",
      "Epoch 12/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6698 - acc: 0.6068 - val_loss: 0.6813 - val_acc: 0.5800\n",
      "Epoch 13/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6698 - acc: 0.6068 - val_loss: 0.6814 - val_acc: 0.5800\n",
      "Epoch 14/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6699 - acc: 0.6068 - val_loss: 0.6814 - val_acc: 0.5800\n",
      "Epoch 15/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6696 - acc: 0.6068 - val_loss: 0.6809 - val_acc: 0.5800\n",
      "Epoch 16/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6693 - acc: 0.6068 - val_loss: 0.6804 - val_acc: 0.5800\n",
      "Epoch 17/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6691 - acc: 0.6068 - val_loss: 0.6803 - val_acc: 0.5800\n",
      "Epoch 18/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6695 - acc: 0.6068 - val_loss: 0.6803 - val_acc: 0.5800\n",
      "Epoch 19/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6694 - acc: 0.6068 - val_loss: 0.6805 - val_acc: 0.5800\n",
      "Epoch 20/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6694 - acc: 0.6068 - val_loss: 0.6808 - val_acc: 0.5800\n",
      "Epoch 21/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6686 - acc: 0.6068 - val_loss: 0.6813 - val_acc: 0.5800\n",
      "Epoch 22/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6687 - acc: 0.6068 - val_loss: 0.6816 - val_acc: 0.5800\n",
      "Epoch 23/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6691 - acc: 0.6068 - val_loss: 0.6817 - val_acc: 0.5800\n",
      "Epoch 24/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6691 - acc: 0.6068 - val_loss: 0.6813 - val_acc: 0.5800\n",
      "Epoch 25/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6690 - acc: 0.6068 - val_loss: 0.6809 - val_acc: 0.5800\n",
      "Epoch 26/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6686 - acc: 0.6068 - val_loss: 0.6807 - val_acc: 0.5800\n",
      "Epoch 27/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6683 - acc: 0.6068 - val_loss: 0.6808 - val_acc: 0.5800\n",
      "Epoch 28/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6686 - acc: 0.6068 - val_loss: 0.6808 - val_acc: 0.5800\n",
      "Epoch 29/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6686 - acc: 0.6068 - val_loss: 0.6808 - val_acc: 0.5800\n",
      "Epoch 30/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6678 - acc: 0.6068 - val_loss: 0.6808 - val_acc: 0.5800\n",
      "Epoch 31/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6679 - acc: 0.6068 - val_loss: 0.6808 - val_acc: 0.5800\n",
      "Epoch 32/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6680 - acc: 0.6068 - val_loss: 0.6806 - val_acc: 0.5800\n",
      "Epoch 33/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6675 - acc: 0.6068 - val_loss: 0.6803 - val_acc: 0.5800\n",
      "Epoch 34/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6671 - acc: 0.6068 - val_loss: 0.6799 - val_acc: 0.5800\n",
      "Epoch 35/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6670 - acc: 0.6068 - val_loss: 0.6797 - val_acc: 0.5800\n",
      "Epoch 36/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6664 - acc: 0.6068 - val_loss: 0.6795 - val_acc: 0.5800\n",
      "Epoch 37/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6672 - acc: 0.6068 - val_loss: 0.6792 - val_acc: 0.5800\n",
      "Epoch 38/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6663 - acc: 0.6068 - val_loss: 0.6792 - val_acc: 0.5800\n",
      "Epoch 39/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6663 - acc: 0.6068 - val_loss: 0.6791 - val_acc: 0.5800\n",
      "Epoch 40/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6665 - acc: 0.6068 - val_loss: 0.6789 - val_acc: 0.5800\n",
      "Epoch 41/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6664 - acc: 0.6068 - val_loss: 0.6784 - val_acc: 0.5800\n",
      "Epoch 42/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6659 - acc: 0.6068 - val_loss: 0.6782 - val_acc: 0.5800\n",
      "Epoch 43/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6660 - acc: 0.6068 - val_loss: 0.6784 - val_acc: 0.5800\n",
      "Epoch 44/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6661 - acc: 0.6068 - val_loss: 0.6786 - val_acc: 0.5800\n",
      "Epoch 45/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6654 - acc: 0.6068 - val_loss: 0.6786 - val_acc: 0.5800\n",
      "Epoch 46/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6647 - acc: 0.6068 - val_loss: 0.6782 - val_acc: 0.5800\n",
      "Epoch 47/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6648 - acc: 0.6068 - val_loss: 0.6777 - val_acc: 0.5800\n",
      "Epoch 48/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6642 - acc: 0.6068 - val_loss: 0.6774 - val_acc: 0.5800\n",
      "Epoch 49/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6633 - acc: 0.6068 - val_loss: 0.6777 - val_acc: 0.5800\n",
      "Epoch 50/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6634 - acc: 0.6068 - val_loss: 0.6768 - val_acc: 0.5800\n",
      "Epoch 51/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6631 - acc: 0.6068 - val_loss: 0.6761 - val_acc: 0.5800\n",
      "Epoch 52/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6619 - acc: 0.6068 - val_loss: 0.6766 - val_acc: 0.5800\n",
      "Epoch 53/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6632 - acc: 0.6068 - val_loss: 0.6761 - val_acc: 0.5800\n",
      "Epoch 54/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6631 - acc: 0.6068 - val_loss: 0.6753 - val_acc: 0.5800\n",
      "Epoch 55/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6629 - acc: 0.6068 - val_loss: 0.6753 - val_acc: 0.5800\n",
      "Epoch 56/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6611 - acc: 0.6068 - val_loss: 0.6754 - val_acc: 0.5800\n",
      "Epoch 57/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6600 - acc: 0.6068 - val_loss: 0.6746 - val_acc: 0.5800\n",
      "Epoch 58/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6597 - acc: 0.6068 - val_loss: 0.6742 - val_acc: 0.5800\n",
      "Epoch 59/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6589 - acc: 0.6068 - val_loss: 0.6741 - val_acc: 0.5800\n",
      "Epoch 60/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6587 - acc: 0.6068 - val_loss: 0.6736 - val_acc: 0.5800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6574 - acc: 0.6068 - val_loss: 0.6732 - val_acc: 0.5800\n",
      "Epoch 62/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6569 - acc: 0.6068 - val_loss: 0.6732 - val_acc: 0.5800\n",
      "Epoch 63/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6570 - acc: 0.6068 - val_loss: 0.6731 - val_acc: 0.5800\n",
      "Epoch 64/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6552 - acc: 0.6068 - val_loss: 0.6730 - val_acc: 0.5800\n",
      "Epoch 65/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6543 - acc: 0.6068 - val_loss: 0.6717 - val_acc: 0.5800\n",
      "Epoch 66/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6545 - acc: 0.6068 - val_loss: 0.6725 - val_acc: 0.5800\n",
      "Epoch 67/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6524 - acc: 0.6068 - val_loss: 0.6710 - val_acc: 0.5800\n",
      "Epoch 68/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6516 - acc: 0.6068 - val_loss: 0.6717 - val_acc: 0.5800\n",
      "Epoch 69/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6538 - acc: 0.6068 - val_loss: 0.6694 - val_acc: 0.5800\n",
      "Epoch 70/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6524 - acc: 0.6068 - val_loss: 0.6701 - val_acc: 0.5800\n",
      "Epoch 71/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6494 - acc: 0.6068 - val_loss: 0.6677 - val_acc: 0.5800\n",
      "Epoch 72/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6494 - acc: 0.6068 - val_loss: 0.6678 - val_acc: 0.5800\n",
      "Epoch 73/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6469 - acc: 0.6068 - val_loss: 0.6662 - val_acc: 0.5800\n",
      "Epoch 74/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6453 - acc: 0.6068 - val_loss: 0.6660 - val_acc: 0.5800\n",
      "Epoch 75/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6452 - acc: 0.6072 - val_loss: 0.6629 - val_acc: 0.5800\n",
      "Epoch 76/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6424 - acc: 0.6108 - val_loss: 0.6695 - val_acc: 0.5800\n",
      "Epoch 77/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6439 - acc: 0.6072 - val_loss: 0.6736 - val_acc: 0.6240\n",
      "Epoch 78/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6610 - acc: 0.6719 - val_loss: 0.8093 - val_acc: 0.5800\n",
      "Epoch 79/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.7662 - acc: 0.6068 - val_loss: 0.7276 - val_acc: 0.4200\n",
      "Epoch 80/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.7318 - acc: 0.3932 - val_loss: 0.6910 - val_acc: 0.5320\n",
      "Epoch 81/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6878 - acc: 0.5840 - val_loss: 0.6794 - val_acc: 0.5800\n",
      "Epoch 82/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6711 - acc: 0.6068 - val_loss: 0.6768 - val_acc: 0.5800\n",
      "Epoch 83/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6649 - acc: 0.6068 - val_loss: 0.6775 - val_acc: 0.5800\n",
      "Epoch 84/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6627 - acc: 0.6068 - val_loss: 0.6799 - val_acc: 0.5800\n",
      "Epoch 85/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6628 - acc: 0.6068 - val_loss: 0.6813 - val_acc: 0.5800\n",
      "Epoch 86/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6635 - acc: 0.6068 - val_loss: 0.6807 - val_acc: 0.5800\n",
      "Epoch 87/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6630 - acc: 0.6068 - val_loss: 0.6792 - val_acc: 0.5800\n",
      "Epoch 88/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6617 - acc: 0.6068 - val_loss: 0.6781 - val_acc: 0.5800\n",
      "Epoch 89/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6617 - acc: 0.6068 - val_loss: 0.6774 - val_acc: 0.5800\n",
      "Epoch 90/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6615 - acc: 0.6068 - val_loss: 0.6772 - val_acc: 0.5800\n",
      "Epoch 91/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6614 - acc: 0.6068 - val_loss: 0.6772 - val_acc: 0.5800\n",
      "Epoch 92/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6609 - acc: 0.6068 - val_loss: 0.6773 - val_acc: 0.5800\n",
      "Epoch 93/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6610 - acc: 0.6068 - val_loss: 0.6773 - val_acc: 0.5800\n",
      "Epoch 94/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6606 - acc: 0.6068 - val_loss: 0.6771 - val_acc: 0.5800\n",
      "Epoch 95/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6598 - acc: 0.6068 - val_loss: 0.6768 - val_acc: 0.5800\n",
      "Epoch 96/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6599 - acc: 0.6068 - val_loss: 0.6763 - val_acc: 0.5800\n",
      "Epoch 97/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6589 - acc: 0.6068 - val_loss: 0.6759 - val_acc: 0.5800\n",
      "Epoch 98/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6590 - acc: 0.6068 - val_loss: 0.6755 - val_acc: 0.5800\n",
      "Epoch 99/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6585 - acc: 0.6068 - val_loss: 0.6751 - val_acc: 0.5800\n",
      "Epoch 100/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6585 - acc: 0.6068 - val_loss: 0.6747 - val_acc: 0.5800\n",
      "Epoch 101/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6578 - acc: 0.6068 - val_loss: 0.6743 - val_acc: 0.5800\n",
      "Epoch 102/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6565 - acc: 0.6068 - val_loss: 0.6738 - val_acc: 0.5800\n",
      "Epoch 103/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6561 - acc: 0.6068 - val_loss: 0.6733 - val_acc: 0.5800\n",
      "Epoch 104/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6558 - acc: 0.6068 - val_loss: 0.6726 - val_acc: 0.5800\n",
      "Epoch 105/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6553 - acc: 0.6068 - val_loss: 0.6720 - val_acc: 0.5800\n",
      "Epoch 106/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6546 - acc: 0.6068 - val_loss: 0.6713 - val_acc: 0.5800\n",
      "Epoch 107/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6535 - acc: 0.6068 - val_loss: 0.6706 - val_acc: 0.5800\n",
      "Epoch 108/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6524 - acc: 0.6068 - val_loss: 0.6698 - val_acc: 0.5800\n",
      "Epoch 109/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6514 - acc: 0.6068 - val_loss: 0.6689 - val_acc: 0.5800\n",
      "Epoch 110/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6506 - acc: 0.6068 - val_loss: 0.6680 - val_acc: 0.5800\n",
      "Epoch 111/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6492 - acc: 0.6068 - val_loss: 0.6671 - val_acc: 0.5800\n",
      "Epoch 112/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6492 - acc: 0.6068 - val_loss: 0.6663 - val_acc: 0.5800\n",
      "Epoch 113/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6479 - acc: 0.6068 - val_loss: 0.6655 - val_acc: 0.5800\n",
      "Epoch 114/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6473 - acc: 0.6068 - val_loss: 0.6649 - val_acc: 0.5800\n",
      "Epoch 115/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6465 - acc: 0.6068 - val_loss: 0.6644 - val_acc: 0.5800\n",
      "Epoch 116/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6451 - acc: 0.6068 - val_loss: 0.6639 - val_acc: 0.5800\n",
      "Epoch 117/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6437 - acc: 0.6068 - val_loss: 0.6635 - val_acc: 0.5800\n",
      "Epoch 118/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6430 - acc: 0.6068 - val_loss: 0.6623 - val_acc: 0.5800\n",
      "Epoch 119/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6416 - acc: 0.6068 - val_loss: 0.6610 - val_acc: 0.5800\n",
      "Epoch 120/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6400 - acc: 0.6068 - val_loss: 0.6601 - val_acc: 0.5800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6398 - acc: 0.6072 - val_loss: 0.6591 - val_acc: 0.5800\n",
      "Epoch 122/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6385 - acc: 0.6068 - val_loss: 0.6579 - val_acc: 0.5800\n",
      "Epoch 123/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6371 - acc: 0.6068 - val_loss: 0.6568 - val_acc: 0.5800\n",
      "Epoch 124/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6352 - acc: 0.6068 - val_loss: 0.6556 - val_acc: 0.5800\n",
      "Epoch 125/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6349 - acc: 0.6068 - val_loss: 0.6544 - val_acc: 0.5800\n",
      "Epoch 126/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6316 - acc: 0.6072 - val_loss: 0.6531 - val_acc: 0.5800\n",
      "Epoch 127/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6297 - acc: 0.6077 - val_loss: 0.6517 - val_acc: 0.5840\n",
      "Epoch 128/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6289 - acc: 0.6081 - val_loss: 0.6503 - val_acc: 0.5880\n",
      "Epoch 129/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6281 - acc: 0.6095 - val_loss: 0.6488 - val_acc: 0.5920\n",
      "Epoch 130/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6268 - acc: 0.6117 - val_loss: 0.6472 - val_acc: 0.5920\n",
      "Epoch 131/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6234 - acc: 0.6157 - val_loss: 0.6457 - val_acc: 0.6040\n",
      "Epoch 132/600\n",
      "2243/2243 [==============================] - 0s 78us/step - loss: 0.6217 - acc: 0.6184 - val_loss: 0.6442 - val_acc: 0.6200\n",
      "Epoch 133/600\n",
      "2243/2243 [==============================] - 0s 78us/step - loss: 0.6206 - acc: 0.6273 - val_loss: 0.6425 - val_acc: 0.6200\n",
      "Epoch 134/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6186 - acc: 0.6331 - val_loss: 0.6407 - val_acc: 0.6240\n",
      "Epoch 135/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6158 - acc: 0.6349 - val_loss: 0.6390 - val_acc: 0.6240\n",
      "Epoch 136/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6136 - acc: 0.6420 - val_loss: 0.6372 - val_acc: 0.6400\n",
      "Epoch 137/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6130 - acc: 0.6420 - val_loss: 0.6351 - val_acc: 0.6480\n",
      "Epoch 138/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6107 - acc: 0.6509 - val_loss: 0.6332 - val_acc: 0.6520\n",
      "Epoch 139/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6072 - acc: 0.6589 - val_loss: 0.6311 - val_acc: 0.6560\n",
      "Epoch 140/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6057 - acc: 0.6647 - val_loss: 0.6289 - val_acc: 0.6640\n",
      "Epoch 141/600\n",
      "2243/2243 [==============================] - 0s 78us/step - loss: 0.6021 - acc: 0.6652 - val_loss: 0.6268 - val_acc: 0.6920\n",
      "Epoch 142/600\n",
      "2243/2243 [==============================] - 0s 78us/step - loss: 0.5989 - acc: 0.6781 - val_loss: 0.6245 - val_acc: 0.6920\n",
      "Epoch 143/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.5984 - acc: 0.6692 - val_loss: 0.6224 - val_acc: 0.7040\n",
      "Epoch 144/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.5937 - acc: 0.6830 - val_loss: 0.6199 - val_acc: 0.6960\n",
      "Epoch 145/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5916 - acc: 0.6835 - val_loss: 0.6181 - val_acc: 0.7000\n",
      "Epoch 146/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.5894 - acc: 0.6959 - val_loss: 0.6157 - val_acc: 0.7120\n",
      "Epoch 147/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.5871 - acc: 0.6884 - val_loss: 0.6137 - val_acc: 0.7040\n",
      "Epoch 148/600\n",
      "2243/2243 [==============================] - 0s 78us/step - loss: 0.5854 - acc: 0.6955 - val_loss: 0.6118 - val_acc: 0.6920\n",
      "Epoch 149/600\n",
      "2243/2243 [==============================] - 0s 78us/step - loss: 0.5828 - acc: 0.6991 - val_loss: 0.6098 - val_acc: 0.7080\n",
      "Epoch 150/600\n",
      "2243/2243 [==============================] - 0s 78us/step - loss: 0.5810 - acc: 0.6977 - val_loss: 0.6079 - val_acc: 0.6960\n",
      "Epoch 151/600\n",
      "2243/2243 [==============================] - 0s 78us/step - loss: 0.5791 - acc: 0.7044 - val_loss: 0.6067 - val_acc: 0.7040\n",
      "Epoch 152/600\n",
      "2243/2243 [==============================] - 0s 78us/step - loss: 0.5758 - acc: 0.7071 - val_loss: 0.6046 - val_acc: 0.6920\n",
      "Epoch 153/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5751 - acc: 0.7053 - val_loss: 0.6047 - val_acc: 0.7000\n",
      "Epoch 154/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5704 - acc: 0.7107 - val_loss: 0.6020 - val_acc: 0.6920\n",
      "Epoch 155/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5720 - acc: 0.7098 - val_loss: 0.6034 - val_acc: 0.7000\n",
      "Epoch 156/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5684 - acc: 0.7075 - val_loss: 0.6004 - val_acc: 0.7040\n",
      "Epoch 157/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5680 - acc: 0.7124 - val_loss: 0.6054 - val_acc: 0.6920\n",
      "Epoch 158/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5651 - acc: 0.7089 - val_loss: 0.5988 - val_acc: 0.7000\n",
      "Epoch 159/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5638 - acc: 0.7178 - val_loss: 0.6112 - val_acc: 0.6560\n",
      "Epoch 160/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5624 - acc: 0.7115 - val_loss: 0.5978 - val_acc: 0.7000\n",
      "Epoch 161/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.5619 - acc: 0.7173 - val_loss: 0.6232 - val_acc: 0.6400\n",
      "Epoch 162/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.5615 - acc: 0.7071 - val_loss: 0.5954 - val_acc: 0.6880\n",
      "Epoch 163/600\n",
      "2243/2243 [==============================] - 0s 78us/step - loss: 0.5613 - acc: 0.7196 - val_loss: 0.6577 - val_acc: 0.6160\n",
      "Epoch 164/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5671 - acc: 0.6977 - val_loss: 0.5926 - val_acc: 0.6960\n",
      "Epoch 165/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.5757 - acc: 0.7017 - val_loss: 0.7450 - val_acc: 0.6000\n",
      "Epoch 166/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5970 - acc: 0.6714 - val_loss: 0.6017 - val_acc: 0.6800\n",
      "Epoch 167/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6107 - acc: 0.6705 - val_loss: 0.8215 - val_acc: 0.5840\n",
      "Epoch 168/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6311 - acc: 0.6549 - val_loss: 0.5919 - val_acc: 0.6920\n",
      "Epoch 169/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5749 - acc: 0.7031 - val_loss: 0.6331 - val_acc: 0.6320\n",
      "Epoch 170/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5535 - acc: 0.7169 - val_loss: 0.6633 - val_acc: 0.6200\n",
      "Epoch 171/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5569 - acc: 0.7102 - val_loss: 0.6037 - val_acc: 0.6720\n",
      "Epoch 172/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5570 - acc: 0.7200 - val_loss: 0.6779 - val_acc: 0.6160\n",
      "Epoch 173/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5524 - acc: 0.7133 - val_loss: 0.6281 - val_acc: 0.6480\n",
      "Epoch 174/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5491 - acc: 0.7240 - val_loss: 0.6676 - val_acc: 0.6200\n",
      "Epoch 175/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.5462 - acc: 0.7196 - val_loss: 0.6578 - val_acc: 0.6160\n",
      "Epoch 176/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5461 - acc: 0.7222 - val_loss: 0.6688 - val_acc: 0.6200\n",
      "Epoch 177/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5442 - acc: 0.7254 - val_loss: 0.6759 - val_acc: 0.6200\n",
      "Epoch 178/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5441 - acc: 0.7240 - val_loss: 0.6785 - val_acc: 0.6200\n",
      "Epoch 179/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5426 - acc: 0.7272 - val_loss: 0.6896 - val_acc: 0.6160\n",
      "Epoch 180/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5421 - acc: 0.7236 - val_loss: 0.6859 - val_acc: 0.6160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5403 - acc: 0.7263 - val_loss: 0.7089 - val_acc: 0.6160\n",
      "Epoch 182/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5393 - acc: 0.7272 - val_loss: 0.6882 - val_acc: 0.6160\n",
      "Epoch 183/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5383 - acc: 0.7289 - val_loss: 0.7336 - val_acc: 0.6080\n",
      "Epoch 184/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5389 - acc: 0.7209 - val_loss: 0.6810 - val_acc: 0.6160\n",
      "Epoch 185/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5375 - acc: 0.7325 - val_loss: 0.7873 - val_acc: 0.5920\n",
      "Epoch 186/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5409 - acc: 0.7200 - val_loss: 0.6446 - val_acc: 0.6360\n",
      "Epoch 187/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5474 - acc: 0.7240 - val_loss: 0.9307 - val_acc: 0.5840\n",
      "Epoch 188/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5705 - acc: 0.7026 - val_loss: 0.5877 - val_acc: 0.7040\n",
      "Epoch 189/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6149 - acc: 0.6594 - val_loss: 1.2153 - val_acc: 0.5840\n",
      "Epoch 190/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.7099 - acc: 0.6447 - val_loss: 0.5909 - val_acc: 0.6840\n",
      "Epoch 191/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5901 - acc: 0.6901 - val_loss: 0.7428 - val_acc: 0.5920\n",
      "Epoch 192/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5440 - acc: 0.7169 - val_loss: 0.7340 - val_acc: 0.5880\n",
      "Epoch 193/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5420 - acc: 0.7222 - val_loss: 0.6347 - val_acc: 0.6320\n",
      "Epoch 194/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5454 - acc: 0.7280 - val_loss: 0.7288 - val_acc: 0.5960\n",
      "Epoch 195/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5388 - acc: 0.7231 - val_loss: 0.7056 - val_acc: 0.6080\n",
      "Epoch 196/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5357 - acc: 0.7365 - val_loss: 0.6714 - val_acc: 0.6160\n",
      "Epoch 197/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5370 - acc: 0.7370 - val_loss: 0.7342 - val_acc: 0.5960\n",
      "Epoch 198/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5345 - acc: 0.7365 - val_loss: 0.7026 - val_acc: 0.6080\n",
      "Epoch 199/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5328 - acc: 0.7347 - val_loss: 0.7086 - val_acc: 0.6080\n",
      "Epoch 200/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5316 - acc: 0.7379 - val_loss: 0.7343 - val_acc: 0.6000\n",
      "Epoch 201/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5306 - acc: 0.7334 - val_loss: 0.7174 - val_acc: 0.6000\n",
      "Epoch 202/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5278 - acc: 0.7365 - val_loss: 0.7397 - val_acc: 0.6000\n",
      "Epoch 203/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5282 - acc: 0.7370 - val_loss: 0.7326 - val_acc: 0.6000\n",
      "Epoch 204/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5274 - acc: 0.7379 - val_loss: 0.7434 - val_acc: 0.6000\n",
      "Epoch 205/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5248 - acc: 0.7361 - val_loss: 0.7477 - val_acc: 0.5960\n",
      "Epoch 206/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5233 - acc: 0.7423 - val_loss: 0.7541 - val_acc: 0.5920\n",
      "Epoch 207/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5239 - acc: 0.7414 - val_loss: 0.7528 - val_acc: 0.5920\n",
      "Epoch 208/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5235 - acc: 0.7428 - val_loss: 0.7668 - val_acc: 0.5880\n",
      "Epoch 209/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5203 - acc: 0.7459 - val_loss: 0.7591 - val_acc: 0.5920\n",
      "Epoch 210/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5205 - acc: 0.7454 - val_loss: 0.7735 - val_acc: 0.5880\n",
      "Epoch 211/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5193 - acc: 0.7441 - val_loss: 0.7682 - val_acc: 0.5920\n",
      "Epoch 212/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5178 - acc: 0.7414 - val_loss: 0.7899 - val_acc: 0.5920\n",
      "Epoch 213/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5152 - acc: 0.7414 - val_loss: 0.7980 - val_acc: 0.5960\n",
      "Epoch 214/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5131 - acc: 0.7445 - val_loss: 0.8133 - val_acc: 0.5960\n",
      "Epoch 215/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5112 - acc: 0.7472 - val_loss: 0.8358 - val_acc: 0.5920\n",
      "Epoch 216/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.5104 - acc: 0.7472 - val_loss: 0.8026 - val_acc: 0.5960\n",
      "Epoch 217/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5089 - acc: 0.7530 - val_loss: 0.8869 - val_acc: 0.5840\n",
      "Epoch 218/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.5112 - acc: 0.7486 - val_loss: 0.7288 - val_acc: 0.6080\n",
      "Epoch 219/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5179 - acc: 0.7405 - val_loss: 1.1125 - val_acc: 0.5840\n",
      "Epoch 220/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5533 - acc: 0.7093 - val_loss: 0.5789 - val_acc: 0.7160\n",
      "Epoch 221/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6774 - acc: 0.6121 - val_loss: 1.7993 - val_acc: 0.5800\n",
      "Epoch 222/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.9777 - acc: 0.6135 - val_loss: 0.6299 - val_acc: 0.6320\n",
      "Epoch 223/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5354 - acc: 0.7445 - val_loss: 0.5884 - val_acc: 0.7120\n",
      "Epoch 224/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6025 - acc: 0.6728 - val_loss: 0.8103 - val_acc: 0.5840\n",
      "Epoch 225/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5542 - acc: 0.6946 - val_loss: 0.7851 - val_acc: 0.5840\n",
      "Epoch 226/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.5550 - acc: 0.6933 - val_loss: 0.6303 - val_acc: 0.6240\n",
      "Epoch 227/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5401 - acc: 0.7481 - val_loss: 0.6123 - val_acc: 0.6320\n",
      "Epoch 228/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5396 - acc: 0.7472 - val_loss: 0.6861 - val_acc: 0.6040\n",
      "Epoch 229/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5240 - acc: 0.7410 - val_loss: 0.7207 - val_acc: 0.6000\n",
      "Epoch 230/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5204 - acc: 0.7441 - val_loss: 0.6485 - val_acc: 0.6320\n",
      "Epoch 231/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5146 - acc: 0.7468 - val_loss: 0.6973 - val_acc: 0.6240\n",
      "Epoch 232/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5091 - acc: 0.7490 - val_loss: 0.7419 - val_acc: 0.6040\n",
      "Epoch 233/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5089 - acc: 0.7503 - val_loss: 0.6952 - val_acc: 0.6320\n",
      "Epoch 234/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5057 - acc: 0.7552 - val_loss: 0.7581 - val_acc: 0.6000\n",
      "Epoch 235/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5056 - acc: 0.7503 - val_loss: 0.7465 - val_acc: 0.6040\n",
      "Epoch 236/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5032 - acc: 0.7521 - val_loss: 0.7543 - val_acc: 0.6000\n",
      "Epoch 237/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5019 - acc: 0.7552 - val_loss: 0.7860 - val_acc: 0.5960\n",
      "Epoch 238/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5010 - acc: 0.7530 - val_loss: 0.7666 - val_acc: 0.5960\n",
      "Epoch 239/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5004 - acc: 0.7588 - val_loss: 0.7913 - val_acc: 0.5960\n",
      "Epoch 240/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4975 - acc: 0.7575 - val_loss: 0.7891 - val_acc: 0.5960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4972 - acc: 0.7557 - val_loss: 0.7933 - val_acc: 0.5960\n",
      "Epoch 242/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4956 - acc: 0.7579 - val_loss: 0.8012 - val_acc: 0.6000\n",
      "Epoch 243/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4955 - acc: 0.7610 - val_loss: 0.7935 - val_acc: 0.5960\n",
      "Epoch 244/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4951 - acc: 0.7601 - val_loss: 0.8078 - val_acc: 0.6000\n",
      "Epoch 245/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4931 - acc: 0.7597 - val_loss: 0.7888 - val_acc: 0.6000\n",
      "Epoch 246/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4908 - acc: 0.7637 - val_loss: 0.8080 - val_acc: 0.6000\n",
      "Epoch 247/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4894 - acc: 0.7668 - val_loss: 0.7916 - val_acc: 0.6000\n",
      "Epoch 248/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4888 - acc: 0.7637 - val_loss: 0.8088 - val_acc: 0.6000\n",
      "Epoch 249/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.4871 - acc: 0.7704 - val_loss: 0.7901 - val_acc: 0.6000\n",
      "Epoch 250/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4863 - acc: 0.7637 - val_loss: 0.8170 - val_acc: 0.6000\n",
      "Epoch 251/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4844 - acc: 0.7740 - val_loss: 0.7763 - val_acc: 0.6000\n",
      "Epoch 252/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4832 - acc: 0.7695 - val_loss: 0.8429 - val_acc: 0.6000\n",
      "Epoch 253/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4836 - acc: 0.7713 - val_loss: 0.7355 - val_acc: 0.6080\n",
      "Epoch 254/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4856 - acc: 0.7686 - val_loss: 0.9269 - val_acc: 0.5840\n",
      "Epoch 255/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4917 - acc: 0.7615 - val_loss: 0.6427 - val_acc: 0.6400\n",
      "Epoch 256/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5112 - acc: 0.7552 - val_loss: 1.2140 - val_acc: 0.5840\n",
      "Epoch 257/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5805 - acc: 0.6919 - val_loss: 0.5761 - val_acc: 0.7200\n",
      "Epoch 258/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6632 - acc: 0.6233 - val_loss: 1.6249 - val_acc: 0.5840\n",
      "Epoch 259/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.8209 - acc: 0.6326 - val_loss: 0.5724 - val_acc: 0.7040\n",
      "Epoch 260/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.5546 - acc: 0.7160 - val_loss: 0.7127 - val_acc: 0.6200\n",
      "Epoch 261/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4848 - acc: 0.7708 - val_loss: 0.7938 - val_acc: 0.6000\n",
      "Epoch 262/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4988 - acc: 0.7552 - val_loss: 0.6145 - val_acc: 0.6400\n",
      "Epoch 263/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4961 - acc: 0.7646 - val_loss: 0.7136 - val_acc: 0.6200\n",
      "Epoch 264/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4838 - acc: 0.7704 - val_loss: 0.7432 - val_acc: 0.6160\n",
      "Epoch 265/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4826 - acc: 0.7708 - val_loss: 0.6737 - val_acc: 0.6320\n",
      "Epoch 266/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4823 - acc: 0.7637 - val_loss: 0.7821 - val_acc: 0.6040\n",
      "Epoch 267/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4778 - acc: 0.7762 - val_loss: 0.7477 - val_acc: 0.6160\n",
      "Epoch 268/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4741 - acc: 0.7740 - val_loss: 0.7668 - val_acc: 0.6040\n",
      "Epoch 269/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4717 - acc: 0.7757 - val_loss: 0.8104 - val_acc: 0.6000\n",
      "Epoch 270/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4702 - acc: 0.7757 - val_loss: 0.7807 - val_acc: 0.6040\n",
      "Epoch 271/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4692 - acc: 0.7780 - val_loss: 0.8334 - val_acc: 0.6000\n",
      "Epoch 272/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4681 - acc: 0.7802 - val_loss: 0.8010 - val_acc: 0.6040\n",
      "Epoch 273/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4657 - acc: 0.7824 - val_loss: 0.8478 - val_acc: 0.6000\n",
      "Epoch 274/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4636 - acc: 0.7798 - val_loss: 0.8070 - val_acc: 0.6040\n",
      "Epoch 275/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4625 - acc: 0.7869 - val_loss: 0.8662 - val_acc: 0.6000\n",
      "Epoch 276/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4628 - acc: 0.7847 - val_loss: 0.7879 - val_acc: 0.6040\n",
      "Epoch 277/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4631 - acc: 0.7838 - val_loss: 0.9182 - val_acc: 0.5840\n",
      "Epoch 278/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4641 - acc: 0.7869 - val_loss: 0.7285 - val_acc: 0.6120\n",
      "Epoch 279/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4705 - acc: 0.7802 - val_loss: 1.0682 - val_acc: 0.5840\n",
      "Epoch 280/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4891 - acc: 0.7490 - val_loss: 0.6107 - val_acc: 0.6320\n",
      "Epoch 281/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5317 - acc: 0.7396 - val_loss: 1.4712 - val_acc: 0.5840\n",
      "Epoch 282/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6557 - acc: 0.6652 - val_loss: 0.5733 - val_acc: 0.7240\n",
      "Epoch 283/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6629 - acc: 0.6255 - val_loss: 1.4591 - val_acc: 0.5840\n",
      "Epoch 284/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.6916 - acc: 0.6549 - val_loss: 0.6051 - val_acc: 0.6480\n",
      "Epoch 285/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.4919 - acc: 0.7695 - val_loss: 0.6591 - val_acc: 0.6280\n",
      "Epoch 286/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4685 - acc: 0.7829 - val_loss: 0.8894 - val_acc: 0.5960\n",
      "Epoch 287/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4896 - acc: 0.7548 - val_loss: 0.6614 - val_acc: 0.6280\n",
      "Epoch 288/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4681 - acc: 0.7873 - val_loss: 0.7124 - val_acc: 0.6200\n",
      "Epoch 289/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4609 - acc: 0.7878 - val_loss: 0.8523 - val_acc: 0.6000\n",
      "Epoch 290/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4633 - acc: 0.7860 - val_loss: 0.7399 - val_acc: 0.6120\n",
      "Epoch 291/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4576 - acc: 0.7882 - val_loss: 0.8447 - val_acc: 0.6040\n",
      "Epoch 292/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4527 - acc: 0.7873 - val_loss: 0.8787 - val_acc: 0.6000\n",
      "Epoch 293/600\n",
      "2243/2243 [==============================] - 0s 75us/step - loss: 0.4500 - acc: 0.7869 - val_loss: 0.8428 - val_acc: 0.6040\n",
      "Epoch 294/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4471 - acc: 0.7927 - val_loss: 0.9553 - val_acc: 0.5840\n",
      "Epoch 295/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4462 - acc: 0.7936 - val_loss: 0.8689 - val_acc: 0.6040\n",
      "Epoch 296/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4452 - acc: 0.7980 - val_loss: 0.9843 - val_acc: 0.5840\n",
      "Epoch 297/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4422 - acc: 0.8012 - val_loss: 0.8856 - val_acc: 0.6000\n",
      "Epoch 298/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4416 - acc: 0.8038 - val_loss: 1.0244 - val_acc: 0.5840\n",
      "Epoch 299/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4410 - acc: 0.7958 - val_loss: 0.8739 - val_acc: 0.6040\n",
      "Epoch 300/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4403 - acc: 0.8034 - val_loss: 1.1191 - val_acc: 0.5840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4431 - acc: 0.7949 - val_loss: 0.8015 - val_acc: 0.6120\n",
      "Epoch 302/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4624 - acc: 0.7856 - val_loss: 1.4920 - val_acc: 0.5840\n",
      "Epoch 303/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5546 - acc: 0.7098 - val_loss: 0.5761 - val_acc: 0.7160\n",
      "Epoch 304/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.7062 - acc: 0.6108 - val_loss: 1.9299 - val_acc: 0.5800\n",
      "Epoch 305/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.9189 - acc: 0.6206 - val_loss: 0.7044 - val_acc: 0.6160\n",
      "Epoch 306/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.4575 - acc: 0.7976 - val_loss: 0.5908 - val_acc: 0.6560\n",
      "Epoch 307/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5220 - acc: 0.7423 - val_loss: 1.0792 - val_acc: 0.5840\n",
      "Epoch 308/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.5194 - acc: 0.7169 - val_loss: 0.8270 - val_acc: 0.6000\n",
      "Epoch 309/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4587 - acc: 0.7963 - val_loss: 0.6342 - val_acc: 0.6240\n",
      "Epoch 310/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.4827 - acc: 0.7842 - val_loss: 0.8180 - val_acc: 0.6040\n",
      "Epoch 311/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.4498 - acc: 0.7949 - val_loss: 0.9539 - val_acc: 0.5840\n",
      "Epoch 312/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4540 - acc: 0.7927 - val_loss: 0.8031 - val_acc: 0.6040\n",
      "Epoch 313/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4449 - acc: 0.7976 - val_loss: 0.8984 - val_acc: 0.6040\n",
      "Epoch 314/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4359 - acc: 0.8029 - val_loss: 1.0277 - val_acc: 0.5840\n",
      "Epoch 315/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4359 - acc: 0.8025 - val_loss: 0.9344 - val_acc: 0.6000\n",
      "Epoch 316/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4317 - acc: 0.8052 - val_loss: 1.0430 - val_acc: 0.5840\n",
      "Epoch 317/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4268 - acc: 0.8110 - val_loss: 1.0442 - val_acc: 0.5840\n",
      "Epoch 318/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4252 - acc: 0.8110 - val_loss: 1.0295 - val_acc: 0.5840\n",
      "Epoch 319/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4234 - acc: 0.8114 - val_loss: 1.0900 - val_acc: 0.5840\n",
      "Epoch 320/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4220 - acc: 0.8119 - val_loss: 1.0490 - val_acc: 0.5840\n",
      "Epoch 321/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4186 - acc: 0.8181 - val_loss: 1.0994 - val_acc: 0.5840\n",
      "Epoch 322/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4174 - acc: 0.8226 - val_loss: 1.0696 - val_acc: 0.5840\n",
      "Epoch 323/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4153 - acc: 0.8181 - val_loss: 1.1154 - val_acc: 0.5840\n",
      "Epoch 324/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4123 - acc: 0.8235 - val_loss: 1.0775 - val_acc: 0.5840\n",
      "Epoch 325/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4118 - acc: 0.8230 - val_loss: 1.1318 - val_acc: 0.5840\n",
      "Epoch 326/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4094 - acc: 0.8252 - val_loss: 1.0679 - val_acc: 0.5840\n",
      "Epoch 327/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4084 - acc: 0.8270 - val_loss: 1.1632 - val_acc: 0.5840\n",
      "Epoch 328/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4072 - acc: 0.8306 - val_loss: 1.0357 - val_acc: 0.5840\n",
      "Epoch 329/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4071 - acc: 0.8275 - val_loss: 1.2523 - val_acc: 0.5840\n",
      "Epoch 330/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4119 - acc: 0.8185 - val_loss: 0.9216 - val_acc: 0.5960\n",
      "Epoch 331/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4278 - acc: 0.8132 - val_loss: 1.5205 - val_acc: 0.5840\n",
      "Epoch 332/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4780 - acc: 0.7463 - val_loss: 0.6726 - val_acc: 0.6120\n",
      "Epoch 333/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5731 - acc: 0.6928 - val_loss: 1.9610 - val_acc: 0.5800\n",
      "Epoch 334/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.7848 - acc: 0.6384 - val_loss: 0.6308 - val_acc: 0.6320\n",
      "Epoch 335/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5564 - acc: 0.7142 - val_loss: 1.2666 - val_acc: 0.5840\n",
      "Epoch 336/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4619 - acc: 0.7664 - val_loss: 0.8742 - val_acc: 0.6000\n",
      "Epoch 337/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4083 - acc: 0.8226 - val_loss: 0.8559 - val_acc: 0.6040\n",
      "Epoch 338/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4124 - acc: 0.8194 - val_loss: 1.1335 - val_acc: 0.5840\n",
      "Epoch 339/600\n",
      "2243/2243 [==============================] - 0s 75us/step - loss: 0.4195 - acc: 0.8078 - val_loss: 0.9133 - val_acc: 0.6040\n",
      "Epoch 340/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4071 - acc: 0.8243 - val_loss: 1.0989 - val_acc: 0.5840\n",
      "Epoch 341/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3988 - acc: 0.8346 - val_loss: 1.1088 - val_acc: 0.5840\n",
      "Epoch 342/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3942 - acc: 0.8408 - val_loss: 1.0775 - val_acc: 0.5840\n",
      "Epoch 343/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3935 - acc: 0.8337 - val_loss: 1.2120 - val_acc: 0.5840\n",
      "Epoch 344/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3914 - acc: 0.8373 - val_loss: 1.0993 - val_acc: 0.5840\n",
      "Epoch 345/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3908 - acc: 0.8391 - val_loss: 1.2560 - val_acc: 0.5840\n",
      "Epoch 346/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3891 - acc: 0.8426 - val_loss: 1.0938 - val_acc: 0.5840\n",
      "Epoch 347/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3887 - acc: 0.8391 - val_loss: 1.3128 - val_acc: 0.5840\n",
      "Epoch 348/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3916 - acc: 0.8373 - val_loss: 1.0241 - val_acc: 0.5880\n",
      "Epoch 349/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3964 - acc: 0.8422 - val_loss: 1.4507 - val_acc: 0.5840\n",
      "Epoch 350/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4140 - acc: 0.7936 - val_loss: 0.8542 - val_acc: 0.6040\n",
      "Epoch 351/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4527 - acc: 0.7864 - val_loss: 1.7465 - val_acc: 0.5800\n",
      "Epoch 352/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.5539 - acc: 0.7035 - val_loss: 0.6632 - val_acc: 0.6320\n",
      "Epoch 353/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.5865 - acc: 0.6848 - val_loss: 1.8296 - val_acc: 0.5800\n",
      "Epoch 354/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6536 - acc: 0.6741 - val_loss: 0.7554 - val_acc: 0.6080\n",
      "Epoch 355/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.4513 - acc: 0.7887 - val_loss: 1.1118 - val_acc: 0.5880\n",
      "Epoch 356/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3855 - acc: 0.8404 - val_loss: 1.1621 - val_acc: 0.5840\n",
      "Epoch 357/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3890 - acc: 0.8417 - val_loss: 0.9397 - val_acc: 0.6040\n",
      "Epoch 358/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3957 - acc: 0.8346 - val_loss: 1.2848 - val_acc: 0.5840\n",
      "Epoch 359/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3882 - acc: 0.8355 - val_loss: 1.1218 - val_acc: 0.5840\n",
      "Epoch 360/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3766 - acc: 0.8444 - val_loss: 1.2358 - val_acc: 0.5840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3723 - acc: 0.8489 - val_loss: 1.2833 - val_acc: 0.5840\n",
      "Epoch 362/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3688 - acc: 0.8556 - val_loss: 1.2242 - val_acc: 0.5840\n",
      "Epoch 363/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3670 - acc: 0.8564 - val_loss: 1.3415 - val_acc: 0.5840\n",
      "Epoch 364/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3663 - acc: 0.8538 - val_loss: 1.2220 - val_acc: 0.5840\n",
      "Epoch 365/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3636 - acc: 0.8636 - val_loss: 1.3786 - val_acc: 0.5840\n",
      "Epoch 366/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3635 - acc: 0.8578 - val_loss: 1.1882 - val_acc: 0.5840\n",
      "Epoch 367/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3621 - acc: 0.8605 - val_loss: 1.4431 - val_acc: 0.5840\n",
      "Epoch 368/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3673 - acc: 0.8498 - val_loss: 1.0818 - val_acc: 0.5960\n",
      "Epoch 369/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3792 - acc: 0.8435 - val_loss: 1.6268 - val_acc: 0.5840\n",
      "Epoch 370/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4149 - acc: 0.7829 - val_loss: 0.8459 - val_acc: 0.6160\n",
      "Epoch 371/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4779 - acc: 0.7606 - val_loss: 1.9719 - val_acc: 0.5800\n",
      "Epoch 372/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6302 - acc: 0.6790 - val_loss: 0.6972 - val_acc: 0.6280\n",
      "Epoch 373/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5736 - acc: 0.6933 - val_loss: 1.7685 - val_acc: 0.5800\n",
      "Epoch 374/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.5440 - acc: 0.7133 - val_loss: 0.9093 - val_acc: 0.6120\n",
      "Epoch 375/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3928 - acc: 0.8279 - val_loss: 1.1342 - val_acc: 0.5960\n",
      "Epoch 376/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3588 - acc: 0.8529 - val_loss: 1.3550 - val_acc: 0.5840\n",
      "Epoch 377/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3718 - acc: 0.8382 - val_loss: 1.0589 - val_acc: 0.6000\n",
      "Epoch 378/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3707 - acc: 0.8538 - val_loss: 1.3795 - val_acc: 0.5840\n",
      "Epoch 379/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3559 - acc: 0.8529 - val_loss: 1.2867 - val_acc: 0.5840\n",
      "Epoch 380/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3468 - acc: 0.8680 - val_loss: 1.2920 - val_acc: 0.5840\n",
      "Epoch 381/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3438 - acc: 0.8720 - val_loss: 1.4196 - val_acc: 0.5840\n",
      "Epoch 382/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3429 - acc: 0.8712 - val_loss: 1.2829 - val_acc: 0.5840\n",
      "Epoch 383/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3405 - acc: 0.8778 - val_loss: 1.4462 - val_acc: 0.5840\n",
      "Epoch 384/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3389 - acc: 0.8703 - val_loss: 1.2739 - val_acc: 0.5840\n",
      "Epoch 385/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3375 - acc: 0.8805 - val_loss: 1.4884 - val_acc: 0.5840\n",
      "Epoch 386/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3382 - acc: 0.8725 - val_loss: 1.2081 - val_acc: 0.5880\n",
      "Epoch 387/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3424 - acc: 0.8743 - val_loss: 1.6121 - val_acc: 0.5840\n",
      "Epoch 388/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3579 - acc: 0.8373 - val_loss: 1.0454 - val_acc: 0.6000\n",
      "Epoch 389/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3867 - acc: 0.8297 - val_loss: 1.8785 - val_acc: 0.5800\n",
      "Epoch 390/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.4694 - acc: 0.7419 - val_loss: 0.7830 - val_acc: 0.6120\n",
      "Epoch 391/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5384 - acc: 0.7129 - val_loss: 2.0914 - val_acc: 0.5800\n",
      "Epoch 392/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6600 - acc: 0.6737 - val_loss: 0.8075 - val_acc: 0.6120\n",
      "Epoch 393/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.4565 - acc: 0.7775 - val_loss: 1.4512 - val_acc: 0.5840\n",
      "Epoch 394/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3584 - acc: 0.8391 - val_loss: 1.2375 - val_acc: 0.5920\n",
      "Epoch 395/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3311 - acc: 0.8761 - val_loss: 1.1105 - val_acc: 0.6000\n",
      "Epoch 396/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3406 - acc: 0.8618 - val_loss: 1.4831 - val_acc: 0.5840\n",
      "Epoch 397/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3419 - acc: 0.8613 - val_loss: 1.2230 - val_acc: 0.5920\n",
      "Epoch 398/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3269 - acc: 0.8778 - val_loss: 1.4284 - val_acc: 0.5840\n",
      "Epoch 399/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3194 - acc: 0.8850 - val_loss: 1.3856 - val_acc: 0.5840\n",
      "Epoch 400/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3133 - acc: 0.8894 - val_loss: 1.3878 - val_acc: 0.5840\n",
      "Epoch 401/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3122 - acc: 0.8966 - val_loss: 1.4701 - val_acc: 0.5840\n",
      "Epoch 402/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3097 - acc: 0.8952 - val_loss: 1.3858 - val_acc: 0.5840\n",
      "Epoch 403/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3073 - acc: 0.9001 - val_loss: 1.5299 - val_acc: 0.5840\n",
      "Epoch 404/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3062 - acc: 0.8943 - val_loss: 1.3510 - val_acc: 0.5840\n",
      "Epoch 405/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3084 - acc: 0.8988 - val_loss: 1.6406 - val_acc: 0.5840\n",
      "Epoch 406/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3145 - acc: 0.8778 - val_loss: 1.2350 - val_acc: 0.5960\n",
      "Epoch 407/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3314 - acc: 0.8707 - val_loss: 1.8880 - val_acc: 0.5800\n",
      "Epoch 408/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3795 - acc: 0.7945 - val_loss: 0.9519 - val_acc: 0.6040\n",
      "Epoch 409/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4605 - acc: 0.7677 - val_loss: 2.2642 - val_acc: 0.5800\n",
      "Epoch 410/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6409 - acc: 0.6772 - val_loss: 0.7825 - val_acc: 0.6200\n",
      "Epoch 411/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5473 - acc: 0.7115 - val_loss: 1.9487 - val_acc: 0.5840\n",
      "Epoch 412/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4768 - acc: 0.7499 - val_loss: 1.0922 - val_acc: 0.6080\n",
      "Epoch 413/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3274 - acc: 0.8654 - val_loss: 1.2401 - val_acc: 0.5960\n",
      "Epoch 414/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3074 - acc: 0.8850 - val_loss: 1.5931 - val_acc: 0.5840\n",
      "Epoch 415/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3265 - acc: 0.8613 - val_loss: 1.2165 - val_acc: 0.6000\n",
      "Epoch 416/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3144 - acc: 0.8841 - val_loss: 1.5347 - val_acc: 0.5840\n",
      "Epoch 417/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2981 - acc: 0.8943 - val_loss: 1.4636 - val_acc: 0.5840\n",
      "Epoch 418/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2891 - acc: 0.9064 - val_loss: 1.4360 - val_acc: 0.5840\n",
      "Epoch 419/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2860 - acc: 0.9099 - val_loss: 1.5724 - val_acc: 0.5840\n",
      "Epoch 420/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2861 - acc: 0.9059 - val_loss: 1.4214 - val_acc: 0.5840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 421/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2833 - acc: 0.9131 - val_loss: 1.6071 - val_acc: 0.5840\n",
      "Epoch 422/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2811 - acc: 0.9068 - val_loss: 1.4206 - val_acc: 0.5880\n",
      "Epoch 423/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2792 - acc: 0.9166 - val_loss: 1.6542 - val_acc: 0.5840\n",
      "Epoch 424/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2803 - acc: 0.9019 - val_loss: 1.3802 - val_acc: 0.5920\n",
      "Epoch 425/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2818 - acc: 0.9131 - val_loss: 1.7606 - val_acc: 0.5840\n",
      "Epoch 426/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2910 - acc: 0.8819 - val_loss: 1.2569 - val_acc: 0.5960\n",
      "Epoch 427/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3099 - acc: 0.8814 - val_loss: 1.9935 - val_acc: 0.5800\n",
      "Epoch 428/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.3620 - acc: 0.8047 - val_loss: 0.9909 - val_acc: 0.6080\n",
      "Epoch 429/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4310 - acc: 0.7833 - val_loss: 2.3114 - val_acc: 0.5800\n",
      "Epoch 430/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.5763 - acc: 0.7035 - val_loss: 0.8218 - val_acc: 0.6200\n",
      "Epoch 431/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5065 - acc: 0.7387 - val_loss: 2.0321 - val_acc: 0.5840\n",
      "Epoch 432/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4404 - acc: 0.7650 - val_loss: 1.1336 - val_acc: 0.6000\n",
      "Epoch 433/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2998 - acc: 0.8801 - val_loss: 1.3368 - val_acc: 0.6000\n",
      "Epoch 434/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2713 - acc: 0.9064 - val_loss: 1.6607 - val_acc: 0.5840\n",
      "Epoch 435/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2887 - acc: 0.8823 - val_loss: 1.2597 - val_acc: 0.6040\n",
      "Epoch 436/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2830 - acc: 0.8975 - val_loss: 1.6358 - val_acc: 0.5840\n",
      "Epoch 437/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2673 - acc: 0.9082 - val_loss: 1.4781 - val_acc: 0.5880\n",
      "Epoch 438/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2564 - acc: 0.9273 - val_loss: 1.5315 - val_acc: 0.5880\n",
      "Epoch 439/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2514 - acc: 0.9322 - val_loss: 1.5970 - val_acc: 0.5840\n",
      "Epoch 440/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2507 - acc: 0.9287 - val_loss: 1.4823 - val_acc: 0.5880\n",
      "Epoch 441/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2490 - acc: 0.9327 - val_loss: 1.6293 - val_acc: 0.5840\n",
      "Epoch 442/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2471 - acc: 0.9278 - val_loss: 1.4592 - val_acc: 0.5960\n",
      "Epoch 443/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2444 - acc: 0.9367 - val_loss: 1.6582 - val_acc: 0.5840\n",
      "Epoch 444/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2427 - acc: 0.9242 - val_loss: 1.4349 - val_acc: 0.5960\n",
      "Epoch 445/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2436 - acc: 0.9358 - val_loss: 1.7410 - val_acc: 0.5840\n",
      "Epoch 446/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2449 - acc: 0.9162 - val_loss: 1.3594 - val_acc: 0.5960\n",
      "Epoch 447/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2558 - acc: 0.9224 - val_loss: 1.9225 - val_acc: 0.5840\n",
      "Epoch 448/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2779 - acc: 0.8770 - val_loss: 1.1473 - val_acc: 0.6040\n",
      "Epoch 449/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3209 - acc: 0.8605 - val_loss: 2.2585 - val_acc: 0.5800\n",
      "Epoch 450/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4280 - acc: 0.7593 - val_loss: 0.8170 - val_acc: 0.6360\n",
      "Epoch 451/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5238 - acc: 0.7182 - val_loss: 2.4946 - val_acc: 0.5800\n",
      "Epoch 452/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.6616 - acc: 0.6848 - val_loss: 0.8257 - val_acc: 0.6400\n",
      "Epoch 453/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3970 - acc: 0.8074 - val_loss: 1.5538 - val_acc: 0.5920\n",
      "Epoch 454/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2611 - acc: 0.8979 - val_loss: 1.4894 - val_acc: 0.6000\n",
      "Epoch 455/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2469 - acc: 0.9126 - val_loss: 1.1450 - val_acc: 0.6040\n",
      "Epoch 456/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2651 - acc: 0.9006 - val_loss: 1.6908 - val_acc: 0.5880\n",
      "Epoch 457/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2507 - acc: 0.9064 - val_loss: 1.4115 - val_acc: 0.6040\n",
      "Epoch 458/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2281 - acc: 0.9349 - val_loss: 1.4960 - val_acc: 0.5960\n",
      "Epoch 459/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2214 - acc: 0.9429 - val_loss: 1.6296 - val_acc: 0.5880\n",
      "Epoch 460/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2225 - acc: 0.9362 - val_loss: 1.4313 - val_acc: 0.6000\n",
      "Epoch 461/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2208 - acc: 0.9407 - val_loss: 1.6310 - val_acc: 0.5920\n",
      "Epoch 462/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2164 - acc: 0.9398 - val_loss: 1.4187 - val_acc: 0.6040\n",
      "Epoch 463/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2144 - acc: 0.9523 - val_loss: 1.5771 - val_acc: 0.5920\n",
      "Epoch 464/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2099 - acc: 0.9434 - val_loss: 1.4350 - val_acc: 0.6000\n",
      "Epoch 465/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2078 - acc: 0.9510 - val_loss: 1.5525 - val_acc: 0.5920\n",
      "Epoch 466/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2046 - acc: 0.9478 - val_loss: 1.4502 - val_acc: 0.5960\n",
      "Epoch 467/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2014 - acc: 0.9554 - val_loss: 1.5738 - val_acc: 0.5920\n",
      "Epoch 468/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1992 - acc: 0.9469 - val_loss: 1.4697 - val_acc: 0.6000\n",
      "Epoch 469/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1987 - acc: 0.9532 - val_loss: 1.6491 - val_acc: 0.5920\n",
      "Epoch 470/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1969 - acc: 0.9478 - val_loss: 1.4582 - val_acc: 0.6000\n",
      "Epoch 471/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1966 - acc: 0.9585 - val_loss: 1.7385 - val_acc: 0.5920\n",
      "Epoch 472/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1981 - acc: 0.9412 - val_loss: 1.3716 - val_acc: 0.6040\n",
      "Epoch 473/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2045 - acc: 0.9478 - val_loss: 1.9024 - val_acc: 0.5880\n",
      "Epoch 474/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2208 - acc: 0.9135 - val_loss: 1.1454 - val_acc: 0.6120\n",
      "Epoch 475/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2573 - acc: 0.9046 - val_loss: 2.2465 - val_acc: 0.5840\n",
      "Epoch 476/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3550 - acc: 0.7994 - val_loss: 0.7387 - val_acc: 0.6720\n",
      "Epoch 477/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.5210 - acc: 0.7205 - val_loss: 2.7648 - val_acc: 0.5800\n",
      "Epoch 478/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.8254 - acc: 0.6549 - val_loss: 0.6749 - val_acc: 0.6960\n",
      "Epoch 479/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4979 - acc: 0.7494 - val_loss: 1.6427 - val_acc: 0.6000\n",
      "Epoch 480/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2716 - acc: 0.8765 - val_loss: 1.3516 - val_acc: 0.6040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2144 - acc: 0.9229 - val_loss: 0.9513 - val_acc: 0.6280\n",
      "Epoch 482/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2484 - acc: 0.9019 - val_loss: 1.6259 - val_acc: 0.6040\n",
      "Epoch 483/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2204 - acc: 0.9140 - val_loss: 1.3756 - val_acc: 0.5960\n",
      "Epoch 484/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1887 - acc: 0.9514 - val_loss: 1.2726 - val_acc: 0.5960\n",
      "Epoch 485/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1921 - acc: 0.9545 - val_loss: 1.5947 - val_acc: 0.6040\n",
      "Epoch 486/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1914 - acc: 0.9407 - val_loss: 1.3003 - val_acc: 0.5960\n",
      "Epoch 487/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1820 - acc: 0.9621 - val_loss: 1.4110 - val_acc: 0.5960\n",
      "Epoch 488/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1744 - acc: 0.9617 - val_loss: 1.3985 - val_acc: 0.5960\n",
      "Epoch 489/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1720 - acc: 0.9621 - val_loss: 1.2907 - val_acc: 0.6040\n",
      "Epoch 490/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1702 - acc: 0.9679 - val_loss: 1.3913 - val_acc: 0.5960\n",
      "Epoch 491/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.1685 - acc: 0.9634 - val_loss: 1.3049 - val_acc: 0.6040\n",
      "Epoch 492/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1645 - acc: 0.9692 - val_loss: 1.3610 - val_acc: 0.6040\n",
      "Epoch 493/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1632 - acc: 0.9675 - val_loss: 1.3443 - val_acc: 0.6040\n",
      "Epoch 494/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1603 - acc: 0.9719 - val_loss: 1.3640 - val_acc: 0.6040\n",
      "Epoch 495/600\n",
      "2243/2243 [==============================] - 0s 75us/step - loss: 0.1585 - acc: 0.9710 - val_loss: 1.3900 - val_acc: 0.6080\n",
      "Epoch 496/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1555 - acc: 0.9719 - val_loss: 1.3790 - val_acc: 0.6040\n",
      "Epoch 497/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1547 - acc: 0.9724 - val_loss: 1.4210 - val_acc: 0.6080\n",
      "Epoch 498/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1516 - acc: 0.9728 - val_loss: 1.4088 - val_acc: 0.6080\n",
      "Epoch 499/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1493 - acc: 0.9741 - val_loss: 1.4374 - val_acc: 0.6080\n",
      "Epoch 500/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1470 - acc: 0.9741 - val_loss: 1.4358 - val_acc: 0.6080\n",
      "Epoch 501/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1453 - acc: 0.9733 - val_loss: 1.4482 - val_acc: 0.6080\n",
      "Epoch 502/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1437 - acc: 0.9768 - val_loss: 1.4670 - val_acc: 0.6080\n",
      "Epoch 503/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1416 - acc: 0.9768 - val_loss: 1.4598 - val_acc: 0.6080\n",
      "Epoch 504/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1400 - acc: 0.9764 - val_loss: 1.4819 - val_acc: 0.6080\n",
      "Epoch 505/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1375 - acc: 0.9795 - val_loss: 1.4659 - val_acc: 0.6080\n",
      "Epoch 506/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1367 - acc: 0.9777 - val_loss: 1.4916 - val_acc: 0.6080\n",
      "Epoch 507/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1337 - acc: 0.9786 - val_loss: 1.4709 - val_acc: 0.6080\n",
      "Epoch 508/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1328 - acc: 0.9804 - val_loss: 1.5031 - val_acc: 0.6080\n",
      "Epoch 509/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.1300 - acc: 0.9808 - val_loss: 1.4828 - val_acc: 0.6080\n",
      "Epoch 510/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1294 - acc: 0.9786 - val_loss: 1.5282 - val_acc: 0.6080\n",
      "Epoch 511/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1269 - acc: 0.9817 - val_loss: 1.4733 - val_acc: 0.6080\n",
      "Epoch 512/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1268 - acc: 0.9799 - val_loss: 1.5700 - val_acc: 0.6080\n",
      "Epoch 513/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1251 - acc: 0.9804 - val_loss: 1.4593 - val_acc: 0.6080\n",
      "Epoch 514/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1234 - acc: 0.9808 - val_loss: 1.6135 - val_acc: 0.6080\n",
      "Epoch 515/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1219 - acc: 0.9799 - val_loss: 1.4018 - val_acc: 0.6040\n",
      "Epoch 516/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1221 - acc: 0.9840 - val_loss: 1.6995 - val_acc: 0.6040\n",
      "Epoch 517/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1250 - acc: 0.9773 - val_loss: 1.2728 - val_acc: 0.6120\n",
      "Epoch 518/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1325 - acc: 0.9755 - val_loss: 1.9097 - val_acc: 0.5880\n",
      "Epoch 519/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1504 - acc: 0.9541 - val_loss: 1.0039 - val_acc: 0.6360\n",
      "Epoch 520/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1940 - acc: 0.9264 - val_loss: 2.4540 - val_acc: 0.5880\n",
      "Epoch 521/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.3335 - acc: 0.7989 - val_loss: 0.8048 - val_acc: 0.6760\n",
      "Epoch 522/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.7808 - acc: 0.6309 - val_loss: 3.8609 - val_acc: 0.5800\n",
      "Epoch 523/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 1.8544 - acc: 0.6139 - val_loss: 0.8652 - val_acc: 0.6880\n",
      "Epoch 524/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1738 - acc: 0.9371 - val_loss: 0.9228 - val_acc: 0.6600\n",
      "Epoch 525/600\n",
      "2243/2243 [==============================] - 0s 75us/step - loss: 0.6014 - acc: 0.7307 - val_loss: 1.8616 - val_acc: 0.5960\n",
      "Epoch 526/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.4199 - acc: 0.8199 - val_loss: 1.3864 - val_acc: 0.6120\n",
      "Epoch 527/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.2390 - acc: 0.8908 - val_loss: 0.7038 - val_acc: 0.6800\n",
      "Epoch 528/600\n",
      "2243/2243 [==============================] - 0s 77us/step - loss: 0.2851 - acc: 0.8770 - val_loss: 1.0530 - val_acc: 0.6280\n",
      "Epoch 529/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1683 - acc: 0.9550 - val_loss: 1.4638 - val_acc: 0.6040\n",
      "Epoch 530/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1994 - acc: 0.9211 - val_loss: 1.0825 - val_acc: 0.6200\n",
      "Epoch 531/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1475 - acc: 0.9688 - val_loss: 1.0091 - val_acc: 0.6320\n",
      "Epoch 532/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1492 - acc: 0.9697 - val_loss: 1.2757 - val_acc: 0.6000\n",
      "Epoch 533/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1373 - acc: 0.9724 - val_loss: 1.1515 - val_acc: 0.6160\n",
      "Epoch 534/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1274 - acc: 0.9813 - val_loss: 0.9912 - val_acc: 0.6240\n",
      "Epoch 535/600\n",
      "2243/2243 [==============================] - 0s 75us/step - loss: 0.1256 - acc: 0.9822 - val_loss: 1.0793 - val_acc: 0.6200\n",
      "Epoch 536/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1199 - acc: 0.9835 - val_loss: 1.0560 - val_acc: 0.6240\n",
      "Epoch 537/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1163 - acc: 0.9835 - val_loss: 0.9769 - val_acc: 0.6400\n",
      "Epoch 538/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1146 - acc: 0.9866 - val_loss: 1.0137 - val_acc: 0.6360\n",
      "Epoch 539/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1116 - acc: 0.9848 - val_loss: 1.0380 - val_acc: 0.6320\n",
      "Epoch 540/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1089 - acc: 0.9875 - val_loss: 1.0334 - val_acc: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 541/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1071 - acc: 0.9871 - val_loss: 1.0628 - val_acc: 0.6280\n",
      "Epoch 542/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1049 - acc: 0.9902 - val_loss: 1.1082 - val_acc: 0.6320\n",
      "Epoch 543/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1033 - acc: 0.9889 - val_loss: 1.1080 - val_acc: 0.6360\n",
      "Epoch 544/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.1016 - acc: 0.9902 - val_loss: 1.1377 - val_acc: 0.6280\n",
      "Epoch 545/600\n",
      "2243/2243 [==============================] - 0s 75us/step - loss: 0.0998 - acc: 0.9889 - val_loss: 1.1546 - val_acc: 0.6240\n",
      "Epoch 546/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0978 - acc: 0.9893 - val_loss: 1.1390 - val_acc: 0.6360\n",
      "Epoch 547/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0961 - acc: 0.9906 - val_loss: 1.1790 - val_acc: 0.6240\n",
      "Epoch 548/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0955 - acc: 0.9906 - val_loss: 1.1883 - val_acc: 0.6240\n",
      "Epoch 549/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0930 - acc: 0.9911 - val_loss: 1.1959 - val_acc: 0.6240\n",
      "Epoch 550/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0914 - acc: 0.9920 - val_loss: 1.2137 - val_acc: 0.6280\n",
      "Epoch 551/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0899 - acc: 0.9924 - val_loss: 1.2239 - val_acc: 0.6280\n",
      "Epoch 552/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0887 - acc: 0.9929 - val_loss: 1.2200 - val_acc: 0.6280\n",
      "Epoch 553/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0877 - acc: 0.9933 - val_loss: 1.2465 - val_acc: 0.6280\n",
      "Epoch 554/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0863 - acc: 0.9924 - val_loss: 1.2367 - val_acc: 0.6280\n",
      "Epoch 555/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0861 - acc: 0.9929 - val_loss: 1.2423 - val_acc: 0.6280\n",
      "Epoch 556/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0844 - acc: 0.9938 - val_loss: 1.2749 - val_acc: 0.6280\n",
      "Epoch 557/600\n",
      "2243/2243 [==============================] - 0s 75us/step - loss: 0.0825 - acc: 0.9942 - val_loss: 1.2608 - val_acc: 0.6280\n",
      "Epoch 558/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0813 - acc: 0.9942 - val_loss: 1.2695 - val_acc: 0.6280\n",
      "Epoch 559/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0806 - acc: 0.9938 - val_loss: 1.2871 - val_acc: 0.6280\n",
      "Epoch 560/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0789 - acc: 0.9951 - val_loss: 1.2941 - val_acc: 0.6280\n",
      "Epoch 561/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0786 - acc: 0.9947 - val_loss: 1.2812 - val_acc: 0.6280\n",
      "Epoch 562/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0764 - acc: 0.9951 - val_loss: 1.3083 - val_acc: 0.6280\n",
      "Epoch 563/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0754 - acc: 0.9951 - val_loss: 1.2899 - val_acc: 0.6280\n",
      "Epoch 564/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0743 - acc: 0.9955 - val_loss: 1.3020 - val_acc: 0.6280\n",
      "Epoch 565/600\n",
      "2243/2243 [==============================] - 0s 75us/step - loss: 0.0730 - acc: 0.9955 - val_loss: 1.3254 - val_acc: 0.6280\n",
      "Epoch 566/600\n",
      "2243/2243 [==============================] - 0s 75us/step - loss: 0.0721 - acc: 0.9964 - val_loss: 1.3148 - val_acc: 0.6320\n",
      "Epoch 567/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0708 - acc: 0.9960 - val_loss: 1.3274 - val_acc: 0.6280\n",
      "Epoch 568/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0697 - acc: 0.9960 - val_loss: 1.3552 - val_acc: 0.6280\n",
      "Epoch 569/600\n",
      "2243/2243 [==============================] - 0s 75us/step - loss: 0.0689 - acc: 0.9969 - val_loss: 1.3299 - val_acc: 0.6320\n",
      "Epoch 570/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0680 - acc: 0.9960 - val_loss: 1.3445 - val_acc: 0.6320\n",
      "Epoch 571/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0674 - acc: 0.9973 - val_loss: 1.3350 - val_acc: 0.6320\n",
      "Epoch 572/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0655 - acc: 0.9973 - val_loss: 1.3443 - val_acc: 0.6320\n",
      "Epoch 573/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0653 - acc: 0.9982 - val_loss: 1.3423 - val_acc: 0.6320\n",
      "Epoch 574/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0641 - acc: 0.9978 - val_loss: 1.3498 - val_acc: 0.6320\n",
      "Epoch 575/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0635 - acc: 0.9978 - val_loss: 1.3473 - val_acc: 0.6320\n",
      "Epoch 576/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0623 - acc: 0.9978 - val_loss: 1.3526 - val_acc: 0.6320\n",
      "Epoch 577/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0618 - acc: 0.9982 - val_loss: 1.3609 - val_acc: 0.6320\n",
      "Epoch 578/600\n",
      "2243/2243 [==============================] - 0s 75us/step - loss: 0.0614 - acc: 0.9982 - val_loss: 1.3674 - val_acc: 0.6320\n",
      "Epoch 579/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0601 - acc: 0.9982 - val_loss: 1.3609 - val_acc: 0.6320\n",
      "Epoch 580/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0601 - acc: 0.9982 - val_loss: 1.3782 - val_acc: 0.6320\n",
      "Epoch 581/600\n",
      "2243/2243 [==============================] - 0s 75us/step - loss: 0.0587 - acc: 0.9982 - val_loss: 1.3742 - val_acc: 0.6320\n",
      "Epoch 582/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0581 - acc: 0.9987 - val_loss: 1.3795 - val_acc: 0.6320\n",
      "Epoch 583/600\n",
      "2243/2243 [==============================] - 0s 75us/step - loss: 0.0577 - acc: 0.9978 - val_loss: 1.3954 - val_acc: 0.6320\n",
      "Epoch 584/600\n",
      "2243/2243 [==============================] - 0s 75us/step - loss: 0.0562 - acc: 0.9987 - val_loss: 1.3807 - val_acc: 0.6320\n",
      "Epoch 585/600\n",
      "2243/2243 [==============================] - 0s 75us/step - loss: 0.0557 - acc: 0.9982 - val_loss: 1.3914 - val_acc: 0.6320\n",
      "Epoch 586/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0550 - acc: 0.9987 - val_loss: 1.3834 - val_acc: 0.6320\n",
      "Epoch 587/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0542 - acc: 0.9987 - val_loss: 1.3904 - val_acc: 0.6280\n",
      "Epoch 588/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0533 - acc: 0.9991 - val_loss: 1.3961 - val_acc: 0.6280\n",
      "Epoch 589/600\n",
      "2243/2243 [==============================] - 0s 75us/step - loss: 0.0529 - acc: 0.9987 - val_loss: 1.3846 - val_acc: 0.6320\n",
      "Epoch 590/600\n",
      "2243/2243 [==============================] - 0s 75us/step - loss: 0.0522 - acc: 0.9991 - val_loss: 1.3954 - val_acc: 0.6280\n",
      "Epoch 591/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0518 - acc: 0.9991 - val_loss: 1.3861 - val_acc: 0.6320\n",
      "Epoch 592/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0507 - acc: 0.9991 - val_loss: 1.3885 - val_acc: 0.6320\n",
      "Epoch 593/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0501 - acc: 0.9987 - val_loss: 1.3988 - val_acc: 0.6320\n",
      "Epoch 594/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0491 - acc: 0.9987 - val_loss: 1.3805 - val_acc: 0.6320\n",
      "Epoch 595/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0488 - acc: 0.9991 - val_loss: 1.3990 - val_acc: 0.6320\n",
      "Epoch 596/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0478 - acc: 0.9991 - val_loss: 1.3805 - val_acc: 0.6320\n",
      "Epoch 597/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0480 - acc: 0.9991 - val_loss: 1.3922 - val_acc: 0.6320\n",
      "Epoch 598/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0472 - acc: 0.9991 - val_loss: 1.3972 - val_acc: 0.6320\n",
      "Epoch 599/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0462 - acc: 0.9991 - val_loss: 1.3997 - val_acc: 0.6320\n",
      "Epoch 600/600\n",
      "2243/2243 [==============================] - 0s 76us/step - loss: 0.0453 - acc: 0.9991 - val_loss: 1.3999 - val_acc: 0.6320\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Concatenate\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Reshape\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, UpSampling2D,Convolution1D,MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint,LearningRateScheduler\n",
    "import os\n",
    "from keras.optimizers import SGD\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "with session.as_default():\n",
    "     with session.graph.as_default():\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        input_shape2=(20,20,1)\n",
    "        input_img = Input(batch_shape=(None, 20,20,1))\n",
    "        input_img1 = Input(batch_shape=(None, 20,20,1))\n",
    "        input_img2 = Input(batch_shape=(None, variaveis,1))\n",
    "\n",
    "\n",
    "        denoise_left=Convolution2D(20, 3,3,\n",
    "                                border_mode='same',\n",
    "                                input_shape=input_shape)(input_img)\n",
    "        denoise_left=BatchNormalization()(denoise_left)\n",
    "        denoise_left=Activation('relu')(denoise_left)\n",
    "        denoise_left=MaxPooling2D(pool_size=(2,2))(denoise_left)\n",
    "        denoise_left=Convolution2D(20, 3, 3,\n",
    "                                    init='glorot_uniform',border_mode='same')(denoise_left)\n",
    "        denoise_left=BatchNormalization()(denoise_left)\n",
    "        denoise_left=Activation('relu')(denoise_left)\n",
    "        denoise_left=Convolution2D(8, 3, 3,init='glorot_uniform',border_mode='same')(denoise_left)\n",
    "        denoise_left=BatchNormalization()(denoise_left)\n",
    "        denoise_left=Activation('relu')(denoise_left)\n",
    "        denoise_left=UpSampling2D(size=(2, 2))(denoise_left)\n",
    "        denoise_left=Convolution2D(8, 3, 3,init='glorot_uniform',border_mode='same')(denoise_left)\n",
    "        denoise_left=BatchNormalization()(denoise_left)\n",
    "        denoise_left=Activation('relu')(denoise_left)\n",
    "        denoise_left=UpSampling2D(size=(2, 2))(denoise_left)\n",
    "        denoise_left=Convolution2D(8, 3, 3,init='glorot_uniform',border_mode='same')(denoise_left)\n",
    "        denoise_left=BatchNormalization()(denoise_left)\n",
    "        denoise_left=Activation('relu')(denoise_left)\n",
    "        denoise_left=Convolution2D(1, 3, 3,init='glorot_uniform',border_mode='same')(denoise_left)\n",
    "        denoise_left=BatchNormalization()(denoise_left)\n",
    "        denoise_left=Activation('sigmoid')(denoise_left)\n",
    "        denoise_left=Dense(20)(denoise_left)\n",
    "        denoise_left=Reshape((-1,20,1))(denoise_left)\n",
    "        denoise_left=Flatten()(denoise_left)\n",
    "\n",
    "        denoise_left1=Convolution2D(20, 3,3,\n",
    "                                border_mode='same',\n",
    "                                input_shape=input_shape)(input_img1)\n",
    "        denoise_left1=BatchNormalization()(denoise_left1)\n",
    "        denoise_left1=Activation('relu')(denoise_left1)\n",
    "        denoise_left1=MaxPooling2D(pool_size=(2,2))(denoise_left1)\n",
    "        denoise_left1=Convolution2D(20, 3, 3,\n",
    "                                    init='glorot_uniform',border_mode='same')(denoise_left1)\n",
    "        denoise_left1=BatchNormalization()(denoise_left1)\n",
    "        denoise_left1=Activation('relu')(denoise_left1)\n",
    "        denoise_left1=Convolution2D(8, 3, 3,init='glorot_uniform',border_mode='same')(denoise_left1)\n",
    "        denoise_left1=BatchNormalization()(denoise_left1)\n",
    "        denoise_left1=Activation('relu')(denoise_left1)\n",
    "        denoise_left1=UpSampling2D(size=(2, 2))(denoise_left1)\n",
    "        denoise_left1=Convolution2D(8, 3, 3,init='glorot_uniform',border_mode='same')(denoise_left1)\n",
    "        denoise_left1=BatchNormalization()(denoise_left1)\n",
    "        denoise_left1=Activation('relu')(denoise_left1)\n",
    "        denoise_left1=UpSampling2D(size=(2, 2))(denoise_left1)\n",
    "        denoise_left1=Convolution2D(8, 3, 3,init='glorot_uniform',border_mode='same')(denoise_left1)\n",
    "        denoise_left1=BatchNormalization()(denoise_left1)\n",
    "        denoise_left1=Activation('relu')(denoise_left1)\n",
    "        denoise_left1=Convolution2D(1, 3, 3,init='glorot_uniform',border_mode='same')(denoise_left1)\n",
    "        denoise_left1=BatchNormalization()(denoise_left1)\n",
    "        denoise_left1=Activation('sigmoid')(denoise_left1)\n",
    "        denoise_left1=Dense(20)(denoise_left1)\n",
    "        denoise_left1=Reshape((-1,20,1))(denoise_left1)\n",
    "        denoise_left1=Flatten()(denoise_left1)\n",
    "\n",
    "        denoise_concat=Dense(20)(input_img2)\n",
    "        denoise_concat=Activation('relu')(denoise_concat)\n",
    "        denoise_concat=Dense(20)(denoise_concat)\n",
    "        denoise_concat=Activation('sigmoid')(denoise_concat)\n",
    "        denoise_concat=Dense(variaveis)(denoise_concat)\n",
    "        denoise_concat=Reshape((-1,variaveis,1))(denoise_concat)\n",
    "        denoise_concat=Flatten()(denoise_concat)\n",
    "\n",
    "        squeeze0=Concatenate()([denoise_left,denoise_left1,denoise_concat])\n",
    "        squeeze0=Dropout(0.1)(squeeze0)\n",
    "        squeeze0=Dense(20)(squeeze0)\n",
    "        squeeze0=Activation('relu')(squeeze0)\n",
    "        squeeze0=Dense(2)(squeeze0)\n",
    "        squeeze0=Activation('sigmoid')(squeeze0)\n",
    "\n",
    "        model = Model(inputs = [input_img,input_img1,input_img2], outputs = squeeze0)\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01,momentum=0.6, decay=5e-5, nesterov=False),metrics = ['accuracy'])\n",
    "        save = ModelCheckpoint('pesos_keras.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "        init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        session.run(init)\n",
    "        model.fit([X_train, X_train1,concat_train],y_train,\n",
    "                nb_epoch=600,\n",
    "                batch_size=2771,verbose=1,validation_split=0.1,callbacks=[save]\n",
    "                 )\n",
    "        preds=model.predict([X_test,X_test1,concat_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model \n",
    "plot_model(model, to_file='model_gif.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/278 [==============================] - 0s 141us/step\n",
      "Restored model, accuracy: 74.82%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('pesos_keras.hdf5')\n",
    "loss,acc = model.evaluate([X_test, X_test1,concat_test],y_test)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04653198, 0.18678865],\n",
       "       [0.09754336, 0.14846945],\n",
       "       [0.04931238, 0.20231482],\n",
       "       [0.08633527, 0.14121622],\n",
       "       [0.04283047, 0.20828727],\n",
       "       [0.29164916, 0.09566882],\n",
       "       [0.09295866, 0.18208301],\n",
       "       [0.16304272, 0.13132164],\n",
       "       [0.31526503, 0.08701658],\n",
       "       [0.10036233, 0.14645162],\n",
       "       [0.36909407, 0.05972186],\n",
       "       [0.05830103, 0.19359303],\n",
       "       [0.3384604 , 0.07213241],\n",
       "       [0.08737695, 0.14131469],\n",
       "       [0.31980813, 0.07022375],\n",
       "       [0.0415799 , 0.20759684],\n",
       "       [0.07122862, 0.16217765],\n",
       "       [0.11568955, 0.14524499],\n",
       "       [0.15295327, 0.14126039],\n",
       "       [0.10711941, 0.13682875],\n",
       "       [0.21072575, 0.1132088 ],\n",
       "       [0.06847632, 0.14938349],\n",
       "       [0.04875109, 0.21363845],\n",
       "       [0.06011391, 0.21905154],\n",
       "       [0.07228336, 0.15723881],\n",
       "       [0.0570713 , 0.22016951],\n",
       "       [0.04046392, 0.21324041],\n",
       "       [0.14052996, 0.13389552],\n",
       "       [0.1069873 , 0.15752873],\n",
       "       [0.09244233, 0.15401208],\n",
       "       [0.06484443, 0.17311454],\n",
       "       [0.06140071, 0.18712303],\n",
       "       [0.06935716, 0.17057729],\n",
       "       [0.18970826, 0.10268041],\n",
       "       [0.07186991, 0.18989295],\n",
       "       [0.10278761, 0.17257854],\n",
       "       [0.15466529, 0.11851966],\n",
       "       [0.1140435 , 0.13110223],\n",
       "       [0.01764941, 0.30597454],\n",
       "       [0.17373982, 0.1239461 ],\n",
       "       [0.14806905, 0.12540042],\n",
       "       [0.059264  , 0.19671834],\n",
       "       [0.19961986, 0.09707338],\n",
       "       [0.16888931, 0.12160826],\n",
       "       [0.12105   , 0.13235104],\n",
       "       [0.06710374, 0.16723463],\n",
       "       [0.16730571, 0.12728778],\n",
       "       [0.0404765 , 0.22037321],\n",
       "       [0.04921055, 0.19015321],\n",
       "       [0.26776522, 0.09853178],\n",
       "       [0.0941866 , 0.1674369 ],\n",
       "       [0.05720833, 0.1866656 ],\n",
       "       [0.04071864, 0.23491433],\n",
       "       [0.06187221, 0.18367788],\n",
       "       [0.03582668, 0.23151982],\n",
       "       [0.0623123 , 0.16009352],\n",
       "       [0.18236569, 0.10313526],\n",
       "       [0.24464896, 0.07966676],\n",
       "       [0.0837799 , 0.15396404],\n",
       "       [0.15004054, 0.12525636],\n",
       "       [0.03335118, 0.21727636],\n",
       "       [0.06438896, 0.20809385],\n",
       "       [0.08413696, 0.18337393],\n",
       "       [0.11426198, 0.14456016],\n",
       "       [0.20831317, 0.10093743],\n",
       "       [0.11234456, 0.1480192 ],\n",
       "       [0.12711549, 0.14872035],\n",
       "       [0.06156066, 0.17644027],\n",
       "       [0.08986914, 0.15581179],\n",
       "       [0.22771531, 0.10942662],\n",
       "       [0.07999676, 0.13929135],\n",
       "       [0.08423463, 0.15712816],\n",
       "       [0.05642945, 0.21182579],\n",
       "       [0.17304155, 0.09198931],\n",
       "       [0.03989869, 0.1991365 ],\n",
       "       [0.09438998, 0.15001327],\n",
       "       [0.15916702, 0.14848936],\n",
       "       [0.1428873 , 0.15372449],\n",
       "       [0.02222946, 0.27142888],\n",
       "       [0.07503054, 0.16858658],\n",
       "       [0.09464598, 0.147015  ],\n",
       "       [0.05535233, 0.17253792],\n",
       "       [0.1628789 , 0.1271568 ],\n",
       "       [0.04885876, 0.21085712],\n",
       "       [0.05951047, 0.16085097],\n",
       "       [0.20728114, 0.09712592],\n",
       "       [0.05156335, 0.22000998],\n",
       "       [0.03813106, 0.24346349],\n",
       "       [0.06997359, 0.1589647 ],\n",
       "       [0.08001006, 0.17195013],\n",
       "       [0.10877493, 0.13496074],\n",
       "       [0.04573122, 0.1840176 ],\n",
       "       [0.09038234, 0.14611647],\n",
       "       [0.06988376, 0.15537041],\n",
       "       [0.09936967, 0.15075338],\n",
       "       [0.07315832, 0.17318717],\n",
       "       [0.06978714, 0.17196321],\n",
       "       [0.1019941 , 0.13092726],\n",
       "       [0.17991   , 0.11348018],\n",
       "       [0.0378159 , 0.20603204],\n",
       "       [0.02699411, 0.24831268],\n",
       "       [0.03365371, 0.20964941],\n",
       "       [0.19266301, 0.10920316],\n",
       "       [0.06767273, 0.15076417],\n",
       "       [0.05423415, 0.18004432],\n",
       "       [0.02549976, 0.28868616],\n",
       "       [0.03432325, 0.27443606],\n",
       "       [0.36866683, 0.07772502],\n",
       "       [0.06963608, 0.15300217],\n",
       "       [0.01938   , 0.304091  ],\n",
       "       [0.06217527, 0.16917783],\n",
       "       [0.1936005 , 0.10415584],\n",
       "       [0.10510445, 0.14038411],\n",
       "       [0.10153973, 0.12239304],\n",
       "       [0.04853255, 0.20487985],\n",
       "       [0.03688595, 0.24995956],\n",
       "       [0.3699978 , 0.0651592 ],\n",
       "       [0.04425207, 0.21777737],\n",
       "       [0.04912928, 0.2153466 ],\n",
       "       [0.12607032, 0.1569334 ],\n",
       "       [0.13756877, 0.11469516],\n",
       "       [0.02939281, 0.2685762 ],\n",
       "       [0.05254146, 0.19824114],\n",
       "       [0.17522624, 0.09202147],\n",
       "       [0.419929  , 0.0649972 ],\n",
       "       [0.07212067, 0.16010106],\n",
       "       [0.0423131 , 0.23503017],\n",
       "       [0.09513617, 0.14579454],\n",
       "       [0.05531076, 0.1819315 ],\n",
       "       [0.25801015, 0.09365258],\n",
       "       [0.03820282, 0.19672427],\n",
       "       [0.06511161, 0.15358657],\n",
       "       [0.12951139, 0.12481412],\n",
       "       [0.18640965, 0.12138522],\n",
       "       [0.1061829 , 0.13393399],\n",
       "       [0.07413936, 0.15746114],\n",
       "       [0.08594868, 0.16897619],\n",
       "       [0.09177724, 0.14345187],\n",
       "       [0.03527904, 0.23489231],\n",
       "       [0.10442024, 0.15437639],\n",
       "       [0.13049552, 0.12425372],\n",
       "       [0.07987043, 0.16188517],\n",
       "       [0.07013997, 0.15520382],\n",
       "       [0.04365623, 0.2087698 ],\n",
       "       [0.10001349, 0.14599657],\n",
       "       [0.06763509, 0.16828558],\n",
       "       [0.06908017, 0.18926707],\n",
       "       [0.16727546, 0.09881783],\n",
       "       [0.10139596, 0.13664651],\n",
       "       [0.24366316, 0.09102604],\n",
       "       [0.04415995, 0.20251128],\n",
       "       [0.05895507, 0.18618321],\n",
       "       [0.26509658, 0.0810923 ],\n",
       "       [0.04707038, 0.2110286 ],\n",
       "       [0.04913121, 0.17474255],\n",
       "       [0.13381475, 0.13555911],\n",
       "       [0.17230609, 0.10559365],\n",
       "       [0.03545782, 0.17986271],\n",
       "       [0.08035305, 0.14886507],\n",
       "       [0.10834852, 0.12556031],\n",
       "       [0.04628569, 0.20121625],\n",
       "       [0.3416626 , 0.06963265],\n",
       "       [0.12655568, 0.13221216],\n",
       "       [0.10782969, 0.13749787],\n",
       "       [0.03390953, 0.23057422],\n",
       "       [0.18975663, 0.09829003],\n",
       "       [0.13459542, 0.13977164],\n",
       "       [0.04137135, 0.22523072],\n",
       "       [0.25597322, 0.07811442],\n",
       "       [0.06053436, 0.18211243],\n",
       "       [0.21568239, 0.10720512],\n",
       "       [0.10632485, 0.13631293],\n",
       "       [0.07574815, 0.1830985 ],\n",
       "       [0.1518049 , 0.10944498],\n",
       "       [0.05839038, 0.15822926],\n",
       "       [0.03716695, 0.21333337],\n",
       "       [0.17356282, 0.1334359 ],\n",
       "       [0.0696418 , 0.1651898 ],\n",
       "       [0.11042252, 0.12697819],\n",
       "       [0.1398929 , 0.13295674],\n",
       "       [0.24202192, 0.09325722],\n",
       "       [0.05764252, 0.18965802],\n",
       "       [0.02290225, 0.30044392],\n",
       "       [0.08098513, 0.18116459],\n",
       "       [0.14695549, 0.1182504 ],\n",
       "       [0.03468987, 0.2394098 ],\n",
       "       [0.06869957, 0.17593169],\n",
       "       [0.14518034, 0.1319696 ],\n",
       "       [0.22340411, 0.10589728],\n",
       "       [0.09693068, 0.15114215],\n",
       "       [0.12933728, 0.12402874],\n",
       "       [0.06180578, 0.19140837],\n",
       "       [0.07250795, 0.15103814],\n",
       "       [0.07785785, 0.15398633],\n",
       "       [0.0926145 , 0.13204235],\n",
       "       [0.0233101 , 0.28477788],\n",
       "       [0.0372805 , 0.23413303],\n",
       "       [0.07518885, 0.16310194],\n",
       "       [0.05837408, 0.23759282],\n",
       "       [0.03699395, 0.22774798],\n",
       "       [0.06596464, 0.16245258],\n",
       "       [0.0791328 , 0.16162711],\n",
       "       [0.07696491, 0.18058798],\n",
       "       [0.09744829, 0.1714828 ],\n",
       "       [0.14755803, 0.14312616],\n",
       "       [0.10551912, 0.12923467],\n",
       "       [0.07165974, 0.18023202],\n",
       "       [0.03355137, 0.23758128],\n",
       "       [0.32155386, 0.07967874],\n",
       "       [0.09521478, 0.13398325],\n",
       "       [0.29179102, 0.09840345],\n",
       "       [0.3036174 , 0.09555599],\n",
       "       [0.0437192 , 0.21285939],\n",
       "       [0.02034664, 0.29608372],\n",
       "       [0.2024748 , 0.10643411],\n",
       "       [0.22634527, 0.0809325 ],\n",
       "       [0.18877453, 0.10667297],\n",
       "       [0.02473697, 0.26524287],\n",
       "       [0.08701369, 0.13564318],\n",
       "       [0.05038005, 0.18855298],\n",
       "       [0.14536086, 0.12165889],\n",
       "       [0.08082393, 0.15432018],\n",
       "       [0.05036443, 0.20931324],\n",
       "       [0.17338157, 0.10335448],\n",
       "       [0.12173346, 0.13463679],\n",
       "       [0.03438011, 0.25649667],\n",
       "       [0.05337179, 0.17319033],\n",
       "       [0.09674749, 0.1630038 ],\n",
       "       [0.05855969, 0.15022019],\n",
       "       [0.06981581, 0.19947624],\n",
       "       [0.05300748, 0.17626396],\n",
       "       [0.06962511, 0.16240841],\n",
       "       [0.07196274, 0.15231651],\n",
       "       [0.03160185, 0.24955174],\n",
       "       [0.10793114, 0.13236368],\n",
       "       [0.0505178 , 0.20420396],\n",
       "       [0.36834472, 0.0638293 ],\n",
       "       [0.11492297, 0.15801102],\n",
       "       [0.15605414, 0.10591146],\n",
       "       [0.05600658, 0.17956993],\n",
       "       [0.25981715, 0.07858995],\n",
       "       [0.14715263, 0.10524616],\n",
       "       [0.02888489, 0.25786626],\n",
       "       [0.1043379 , 0.12832394],\n",
       "       [0.0732097 , 0.15292105],\n",
       "       [0.03296089, 0.21396491],\n",
       "       [0.04884982, 0.2033664 ],\n",
       "       [0.18511882, 0.13749352],\n",
       "       [0.06421781, 0.17973408],\n",
       "       [0.12512615, 0.13532299],\n",
       "       [0.09314123, 0.1438489 ],\n",
       "       [0.07327235, 0.17260501],\n",
       "       [0.1269153 , 0.14027801],\n",
       "       [0.07884473, 0.16049677],\n",
       "       [0.09105644, 0.15054941],\n",
       "       [0.07828549, 0.15497482],\n",
       "       [0.06447002, 0.19579965],\n",
       "       [0.10770461, 0.1334551 ],\n",
       "       [0.08283922, 0.15641242],\n",
       "       [0.10988298, 0.12288401],\n",
       "       [0.05400079, 0.24898359],\n",
       "       [0.07869047, 0.16385266],\n",
       "       [0.10781968, 0.14258137],\n",
       "       [0.38415143, 0.07168418],\n",
       "       [0.04833898, 0.2073907 ],\n",
       "       [0.06755501, 0.21359599],\n",
       "       [0.04554823, 0.22034046],\n",
       "       [0.09404069, 0.1339665 ],\n",
       "       [0.2566713 , 0.096416  ],\n",
       "       [0.13099611, 0.13330409],\n",
       "       [0.07703477, 0.16862574],\n",
       "       [0.13220659, 0.12303835],\n",
       "       [0.03093484, 0.2842098 ],\n",
       "       [0.07465827, 0.18444559],\n",
       "       [0.06431404, 0.18083578],\n",
       "       [0.1459766 , 0.12918961],\n",
       "       [0.07029426, 0.14453816],\n",
       "       [0.09551707, 0.13783386]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds00=model.predict([X_test, X_test1,concat_test])\n",
    "preds00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,f1_score,recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Accuracy: 0.7482014388489209\n",
      "Precision (FP): 0.7524271844660194\n",
      "Recall (FN): 0.8908045977011494\n",
      "f1: 0.8157894736842105\n",
      "[[ 53  51]\n",
      " [ 19 155]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix(y_test.T[1],np.argmax(preds00,axis=1))\n",
    "pred0=np.argmax(preds00,axis=1)\n",
    "Y=y_test.T[1]\n",
    "print('\\n')\n",
    "print('Accuracy:',accuracy_score(np.array(Y),pred0))\n",
    "print('Precision (FP):',precision_score(np.array(Y),pred0,average='binary'))\n",
    "print('Recall (FN):',recall_score(np.array(Y),pred0,average='binary'))\n",
    "print('f1:',f1_score(np.array(Y),pred0,average='binary'))\n",
    "print(confusion_matrix(np.array(Y),pred0),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
